{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssheikhorg/dissertation/blob/dev/models_evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5VI43eYCEzw"
      },
      "source": [
        "# Install all the packages needed and import them all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "o6NKXbxIB_C0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e2bc36-4cc3-4d3f-e9eb-c7fdadbf04ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.8.15 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "uv 0.8.15\n",
            "Python 3.12.11\n"
          ]
        }
      ],
      "source": [
        "# Install uv in Colab's default environment\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "!uv --version\n",
        "\n",
        "# get python version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wcoXxLJXdwYz"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q -U \\\n",
        "  numpy==2.0.2 \\\n",
        "  scikit-learn==1.5.2 \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  pillow \\\n",
        "  sentence-transformers \\\n",
        "  faiss-cpu \\\n",
        "  datasets \\\n",
        "  pandas \\\n",
        "  matplotlib \\\n",
        "  seaborn \\\n",
        "  plotly \\\n",
        "  tqdm \\\n",
        "  openai \\\n",
        "  anthropic \\\n",
        "  boto3 \\\n",
        "  langchain \\\n",
        "  langchain-huggingface \\\n",
        "  huggingface_hub \\\n",
        "  langchain-community \\\n",
        "  transformers_stream_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and add global variables"
      ],
      "metadata": {
        "id": "TG8k8c6rBtHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Login to Hugging Face with your token\n",
        "login(token=userdata.get('HF_TOKEN'))\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import json\n",
        "import re\n",
        "import glob\n",
        "import base64\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "# Fixed imports for LangChain compatibility\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "import openai\n",
        "from anthropic import Anthropic\n",
        "import boto3\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Configuration\n",
        "TEMPERATURE = 0.3\n",
        "MAX_TOKENS = 1000\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ENABLE_RAG = True\n",
        "RAG_TOP_K = 3\n",
        "\n",
        "# Medical Test Data\n",
        "MEDICAL_PROMPTS = [\n",
        "    {\n",
        "        \"question\": \"What are the common symptoms of diabetes?\",\n",
        "        \"reference\": \"Common diabetes symptoms include increased thirst, frequent urination, extreme fatigue, blurred vision, and slow healing of cuts or wounds.\",\n",
        "        \"category\": \"endocrinology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does aspirin work in the body?\",\n",
        "        \"reference\": \"Aspirin works by inhibiting cyclooxygenase enzymes, reducing the production of prostaglandins that cause pain, inflammation, and fever. It also has antiplatelet effects.\",\n",
        "        \"category\": \"pharmacology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is hypertension and what are its risk factors?\",\n",
        "        \"reference\": \"Hypertension, or high blood pressure, is a condition where the force of blood against artery walls is too high. Risk factors include age, family history, obesity, lack of exercise, tobacco use, high sodium diet, and stress.\",\n",
        "        \"category\": \"cardiology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the main functions of the liver?\",\n",
        "        \"reference\": \"The liver performs several vital functions including detoxification of chemicals, protein synthesis, production of biochemicals necessary for digestion, glycogen storage, and decomposition of red blood cells.\",\n",
        "        \"category\": \"gastroenterology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the common symptoms of COVID-19?\",\n",
        "        \"reference\": \"Common COVID-19 symptoms include fever, cough, shortness of breath, fatigue, muscle aches, loss of taste or smell, sore throat, and headache.\",\n",
        "        \"category\": \"infectious_disease\"\n",
        "    }\n",
        "]\n",
        "\n",
        "MEDICAL_DATASETS = {\n",
        "    \"pubmedqa\": [\n",
        "        {\n",
        "            \"question\": \"What is the first-line treatment for hypertension?\",\n",
        "            \"reference\": \"First-line treatments for hypertension include thiazide diuretics, ACE inhibitors, angiotensin II receptor blockers, and calcium channel blockers.\",\n",
        "            \"category\": \"cardiology\",\n",
        "            \"dataset\": \"pubmedqa\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How does metformin work in type 2 diabetes?\",\n",
        "            \"reference\": \"Metformin decreases hepatic glucose production, reduces intestinal glucose absorption, and improves insulin sensitivity.\",\n",
        "            \"category\": \"endocrinology\",\n",
        "            \"dataset\": \"pubmedqa\"\n",
        "        }\n",
        "    ],\n",
        "    \"medqa\": [\n",
        "        {\n",
        "            \"question\": \"A 45-year-old patient presents with chest pain radiating to the left arm. What is the most likely diagnosis?\",\n",
        "            \"reference\": \"Chest pain radiating to the left arm is characteristic of myocardial infarction and requires immediate cardiac evaluation.\",\n",
        "            \"category\": \"cardiology\",\n",
        "            \"dataset\": \"medqa\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the gold standard test for diagnosing pulmonary embolism?\",\n",
        "            \"reference\": \"CT pulmonary angiography is the gold standard for diagnosing pulmonary embolism.\",\n",
        "            \"category\": \"pulmonology\",\n",
        "            \"dataset\": \"medqa\"\n",
        "        }\n",
        "    ],\n",
        "    \"mimic_cxr\": [\n",
        "        {\n",
        "            \"question\": \"Describe the findings in a chest X-ray showing cardiomegaly and pulmonary edema.\",\n",
        "            \"reference\": \"Cardiomegaly appears as an enlarged cardiac silhouette, while pulmonary edema manifests as bilateral interstitial opacities and Kerley B lines.\",\n",
        "            \"category\": \"radiology\",\n",
        "            \"dataset\": \"mimic_cxr\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What radiographic signs suggest pneumothorax?\",\n",
        "            \"reference\": \"Pneumothorax is characterized by a visible visceral pleural edge, absence of lung markings peripheral to this edge, and possible mediastinal shift.\",\n",
        "            \"category\": \"radiology\",\n",
        "            \"dataset\": \"mimic_cxr\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Medical knowledge base for fact checking\n",
        "MEDICAL_KNOWLEDGE_BASE = {\n",
        "    \"diabetes\": [\"increased thirst\", \"frequent urination\", \"fatigue\", \"blurred vision\", \"slow healing\", \"metformin\", \"insulin\"],\n",
        "    \"aspirin\": [\"pain relief\", \"anti-inflammatory\", \"blood thinner\", \"fever reducer\", \"inhibit cyclooxygenase\", \"myocardial infarction\"],\n",
        "    \"hypertension\": [\"high blood pressure\", \"silent killer\", \"cardiovascular risk\", \"artery damage\", \"ACE inhibitors\", \"beta blockers\"],\n",
        "    \"liver\": [\"detoxification\", \"protein synthesis\", \"bile production\", \"glycogen storage\", \"jaundice\", \"cirrhosis\"],\n",
        "    \"covid\": [\"fever\", \"cough\", \"shortness of breath\", \"loss of taste/smell\", \"coronavirus\", \"pandemic\"],\n",
        "    \"cardiology\": [\"myocardial infarction\", \"angina\", \"arrhythmia\", \"ECG\", \"troponin\", \"stent\"],\n",
        "    \"pulmonology\": [\"asthma\", \"COPD\", \"pneumonia\", \"spirometry\", \"bronchodilator\", \"oxygen therapy\"],\n",
        "    \"radiology\": [\"x-ray\", \"CT scan\", \"MRI\", \"ultrasound\", \"contrast\", \"radiation\"]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfw-53lCBerp",
        "outputId": "2706fc3f-01a7-4126-9963-e0e3451e6925"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Configuration Functions"
      ],
      "metadata": {
        "id": "92SSTtj3B1CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_configs():\n",
        "    \"\"\"Return model configurations with their HuggingFace IDs and quantization requirements\"\"\"\n",
        "    return {\n",
        "        \"llama-2-7b\": {\n",
        "            \"model_id\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "            \"requires_special_handling\": False,\n",
        "            \"quantization_support\": True\n",
        "        },\n",
        "        \"mistral-7b\": {\n",
        "            \"model_id\": \"mistralai/Mistral-7B-v0.1\",\n",
        "            \"requires_special_handling\": False,\n",
        "            \"quantization_support\": True\n",
        "        },\n",
        "        \"qwen-7b\": {\n",
        "            \"model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "            \"requires_special_handling\": True,\n",
        "            \"quantization_support\": True,\n",
        "            \"padding_side\": \"left\",\n",
        "            \"trust_remote_code\": True\n",
        "        },\n",
        "        \"meditron-7b\": {\n",
        "            \"model_id\": \"epfl-llm/meditron-7b\",\n",
        "            \"requires_special_handling\": False,\n",
        "            \"quantization_support\": True\n",
        "        },\n",
        "        \"biomedgpt\": {\n",
        "            \"model_id\": \"stanford-crfm/BioMedLM\",\n",
        "            \"requires_special_handling\": False,\n",
        "            \"quantization_support\": True\n",
        "        },\n",
        "        \"gpt-oss-20b\": {\n",
        "            \"model_id\": \"openai/gpt-oss-20b\",\n",
        "            \"requires_special_handling\": True,\n",
        "            \"quantization_support\": False,  # This model uses different quantization\n",
        "            \"trust_remote_code\": True\n",
        "        },\n",
        "        \"claude-3.7-sonnet\": {\n",
        "            \"model_id\": \"reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B\",\n",
        "            \"requires_special_handling\": True,\n",
        "            \"quantization_support\": True,\n",
        "            \"trust_remote_code\": True\n",
        "        },\n",
        "        \"grok-2\": {\n",
        "            \"model_id\": \"xai-org/grok-2\",\n",
        "            \"requires_special_handling\": True,\n",
        "            \"quantization_support\": True,\n",
        "            \"trust_remote_code\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def load_local_model(model_name, **kwargs):\n",
        "    \"\"\"Load a local model with pattern matching and proper error handling\"\"\"\n",
        "    model_configs = get_model_configs()\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "\n",
        "    config = model_configs[model_name]\n",
        "    model_id = config[\"model_id\"]\n",
        "\n",
        "    try:\n",
        "        # Common tokenizer parameters\n",
        "        tokenizer_kwargs = {\n",
        "            \"use_fast\": True,\n",
        "            \"trust_remote_code\": config.get(\"trust_remote_code\", False)\n",
        "        }\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n",
        "\n",
        "        # Handle different model types with pattern matching\n",
        "        if \"qwen\" in model_name.lower():\n",
        "            # Qwen specific handling\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            if tokenizer.pad_token_id is None:\n",
        "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            tokenizer.padding_side = config.get(\"padding_side\", \"left\")\n",
        "\n",
        "        elif \"gpt-oss\" in model_name.lower():\n",
        "            # GPT-OSS-20B specific handling - this model uses different quantization\n",
        "            # Don't use BitsAndBytesConfig for this model\n",
        "            quantization_config = None\n",
        "\n",
        "        elif \"claude\" in model_name.lower():\n",
        "            # Claude model handling\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            if tokenizer.pad_token_id is None:\n",
        "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        else:\n",
        "            # Default handling for other models\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            if tokenizer.pad_token_id is None:\n",
        "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        # Handle quantization configuration\n",
        "        if config[\"quantization_support\"] and torch.cuda.is_available() and \"gpt-oss\" not in model_name.lower():\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\"\n",
        "            )\n",
        "        else:\n",
        "            quantization_config = None\n",
        "\n",
        "        # Model loading parameters\n",
        "        model_kwargs = {\n",
        "            \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
        "            \"dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            \"trust_remote_code\": config.get(\"trust_remote_code\", False),\n",
        "            \"low_cpu_mem_usage\": True,\n",
        "        }\n",
        "\n",
        "        # Add quantization config if applicable\n",
        "        if quantization_config:\n",
        "            model_kwargs[\"quantization_config\"] = quantization_config\n",
        "\n",
        "        # Load model\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "\n",
        "        # Post-processing for specific models\n",
        "        if \"qwen\" in model_name.lower():\n",
        "            model.config.pad_token_id = tokenizer.pad_token_id\n",
        "            if hasattr(model, 'transformer'):\n",
        "                model.transformer.padding_idx = tokenizer.pad_token_id\n",
        "\n",
        "        # Generation parameters\n",
        "        generation_kwargs = {\n",
        "            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", 384),\n",
        "            \"temperature\": kwargs.get(\"temperature\", TEMPERATURE),\n",
        "            \"do_sample\": kwargs.get(\"do_sample\", True),\n",
        "            \"truncation\": kwargs.get(\"truncation\", True),\n",
        "        }\n",
        "\n",
        "        # Add token IDs if available\n",
        "        if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:\n",
        "            generation_kwargs[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "        if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:\n",
        "            generation_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id\n",
        "\n",
        "        # Create pipeline\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            **generation_kwargs\n",
        "        )\n",
        "\n",
        "        print(f\"Successfully loaded {model_name}\")\n",
        "        return pipe\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # Return fallback function\n",
        "        def fallback_pipeline(prompt, **kwargs):\n",
        "            return [{'generated_text': f\"Model {model_name} could not be loaded: {str(e)}\"}]\n",
        "        return fallback_pipeline\n",
        "\n",
        "\n",
        "def load_model_by_category(model_name):\n",
        "    \"\"\"Load model based on its category\"\"\"\n",
        "    categories = get_model_categories()\n",
        "\n",
        "    if model_name in categories[\"local_models\"]:\n",
        "        return load_local_model(model_name)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not found in any category\")\n",
        "\n",
        "# Model category handlers using pattern matching\n",
        "MODEL_HANDLERS = {\n",
        "    \"qwen\": {\n",
        "        \"tokenizer_config\": {\n",
        "            \"padding_side\": \"left\",\n",
        "            \"trust_remote_code\": True\n",
        "        },\n",
        "        \"post_processing\": lambda model, tokenizer: setattr(model.config, 'pad_token_id', tokenizer.pad_token_id)\n",
        "    },\n",
        "    \"gpt-oss\": {\n",
        "        \"quantization\": False,\n",
        "        \"trust_remote_code\": True\n",
        "    },\n",
        "    \"claude\": {\n",
        "        \"trust_remote_code\": True\n",
        "    },\n",
        "    \"default\": {\n",
        "        \"quantization\": True,\n",
        "        \"trust_remote_code\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "def get_model_handler(model_name):\n",
        "    \"\"\"Get the appropriate handler for a model based on pattern matching\"\"\"\n",
        "    model_name_lower = model_name.lower()\n",
        "\n",
        "    for pattern, handler in MODEL_HANDLERS.items():\n",
        "        if pattern in model_name_lower:\n",
        "            return handler\n",
        "\n",
        "    return MODEL_HANDLERS[\"default\"]"
      ],
      "metadata": {
        "id": "NDh26IASBoOM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Functions"
      ],
      "metadata": {
        "id": "i_8vK4YxB9RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_medical_retriever():\n",
        "    \"\"\"Create a medical knowledge retriever for RAG\"\"\"\n",
        "    medical_knowledge = [\n",
        "        \"Diabetes symptoms include increased thirst, frequent urination, fatigue, blurred vision.\",\n",
        "        \"Aspirin is a nonsteroidal anti-inflammatory drug that reduces pain and inflammation.\",\n",
        "        \"Hypertension (high blood pressure) is a condition where blood pressure is consistently too high.\",\n",
        "        \"The liver performs detoxification, protein synthesis, and produces biochemicals for digestion.\",\n",
        "        \"COVID-19 symptoms include fever, cough, shortness of breath, fatigue, and loss of taste/smell.\",\n",
        "        \"Antibiotics treat bacterial infections but are ineffective against viral infections.\",\n",
        "        \"Vaccines stimulate the immune system to produce antibodies against specific diseases.\",\n",
        "        \"Cancer treatments include surgery, chemotherapy, radiation therapy, and immunotherapy.\",\n",
        "        \"Heart disease risk factors include high blood pressure, high cholesterol, smoking, and diabetes.\",\n",
        "        \"Mental health conditions like depression can be treated with therapy and medication.\"\n",
        "    ]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500, chunk_overlap=50\n",
        "    )\n",
        "    documents = text_splitter.create_documents(medical_knowledge)\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "    )\n",
        "\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "ywBGHw3dB7Qc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response Generation Functions"
      ],
      "metadata": {
        "id": "R-BsYLrECHi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_local(model, prompt, use_rag=True):\n",
        "    \"\"\"Generate response using local model with error handling\"\"\"\n",
        "    try:\n",
        "        if use_rag:\n",
        "            vectorstore = create_medical_retriever()\n",
        "            docs = vectorstore.similarity_search(prompt, k=RAG_TOP_K)\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            prompt_template = \"\"\"You are a medical AI assistant. Use the following medical context to answer the question accurately and factually.\n",
        "            If you don't know the answer based on the context, say you don't know. Be concise and avoid speculation.\n",
        "\n",
        "            Medical Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Answer:\"\"\"\n",
        "\n",
        "            formatted_prompt = prompt_template.format(context=context, question=prompt)\n",
        "        else:\n",
        "            formatted_prompt = prompt\n",
        "\n",
        "        # Clear memory before generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Generate response with proper error handling\n",
        "        try:\n",
        "            # Try the standard approach first\n",
        "            generation_args = {\n",
        "                'max_new_tokens': 384,\n",
        "                'do_sample': True,\n",
        "                'temperature': TEMPERATURE,\n",
        "                'truncation': True,\n",
        "            }\n",
        "\n",
        "            # Add padding token ID if available\n",
        "            if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'pad_token_id'):\n",
        "                generation_args['pad_token_id'] = model.tokenizer.pad_token_id\n",
        "\n",
        "            response = model(formatted_prompt, **generation_args)[0]['generated_text']\n",
        "\n",
        "        except Exception as gen_error:\n",
        "            print(f\"Generation error: {str(gen_error)}\")\n",
        "            # Try alternative approach without padding\n",
        "            try:\n",
        "                response = model(\n",
        "                    formatted_prompt,\n",
        "                    max_length=512,\n",
        "                    do_sample=True,\n",
        "                    temperature=TEMPERATURE,\n",
        "                    truncation=True\n",
        "                )[0]['generated_text']\n",
        "            except Exception as alt_error:\n",
        "                print(f\"Alternative generation also failed: {str(alt_error)}\")\n",
        "                return f\"Error in text generation: {str(alt_error)}\"\n",
        "\n",
        "        # Extract answer if using RAG\n",
        "        if use_rag and \"Answer:\" in response:\n",
        "            response = response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # Clear memory after generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generation: {str(e)}\")\n",
        "        # Try to recover memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "def generate_response_cloud(model_name, prompt, use_rag=True):\n",
        "    \"\"\"Generate response using cloud model with optional RAG\"\"\"\n",
        "    try:\n",
        "        if use_rag:\n",
        "            vectorstore = create_medical_retriever()\n",
        "            docs = vectorstore.similarity_search(prompt, k=RAG_TOP_K)\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            prompt_template = \"\"\"You are a medical AI assistant. Use the following medical context to answer the question accurately and factually.\n",
        "            If you don't know the answer based on the context, say you don't know. Be concise and avoid speculation.\n",
        "\n",
        "            Medical Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Answer:\"\"\"\n",
        "\n",
        "            formatted_prompt = prompt_template.format(context=context, question=prompt)\n",
        "        else:\n",
        "            formatted_prompt = prompt\n",
        "\n",
        "        response = query_cloud_model(model_name, formatted_prompt)\n",
        "        return response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generation: {str(e)}\")\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "def generate_response(model, prompt, use_rag=True, model_name=\"\"):\n",
        "    \"\"\"Generate response using local model only\"\"\"\n",
        "    return generate_response_local(model, prompt, use_rag)"
      ],
      "metadata": {
        "id": "SaRZF1cxCF7r"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Functions"
      ],
      "metadata": {
        "id": "EgOzhDeHCMz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_semantic_similarity(reference, response):\n",
        "    \"\"\"Calculate semantic similarity between reference and response\"\"\"\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform([reference, response])\n",
        "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        return max(similarity, 0)\n",
        "    except:\n",
        "        return 0.5\n",
        "\n",
        "def check_factual_consistency(response, category):\n",
        "    \"\"\"Check factual consistency with medical knowledge base\"\"\"\n",
        "    if category not in MEDICAL_KNOWLEDGE_BASE:\n",
        "        return 0.7\n",
        "\n",
        "    relevant_facts = MEDICAL_KNOWLEDGE_BASE[category]\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    fact_matches = sum(1 for fact in relevant_facts if fact in response_lower)\n",
        "\n",
        "    if not relevant_facts:\n",
        "        return 0.5\n",
        "\n",
        "    coverage = fact_matches / len(relevant_facts)\n",
        "\n",
        "    contradictions = check_contradictions(response_lower, category)\n",
        "\n",
        "    final_score = coverage * (1 - 0.5 * contradictions)\n",
        "\n",
        "    return max(min(final_score, 1.0), 0.0)\n",
        "\n",
        "def check_contradictions(response, category):\n",
        "    \"\"\"Check for contradictions with known medical facts\"\"\"\n",
        "    contradiction_patterns = {\n",
        "        \"diabetes\": [r\"diabetes.*curable\", r\"diabetes.*not serious\", r\"insulin.*addictive\"],\n",
        "        \"aspirin\": [r\"aspirin.*safe for everyone\", r\"aspirin.*no side effects\", r\"aspirin.*cures\"],\n",
        "        \"covid\": [r\"covid.*just flu\", r\"vaccines.*dangerous\", r\"masks.*don't work\"],\n",
        "        \"cancer\": [r\"cancer.*always fatal\", r\"alternative.*cures cancer\", r\"chemotherapy.*poison\"]\n",
        "    }\n",
        "\n",
        "    if category not in contradiction_patterns:\n",
        "        return 0.0\n",
        "\n",
        "    patterns = contradiction_patterns[category]\n",
        "    contradiction_count = sum(1 for pattern in patterns if re.search(pattern, response))\n",
        "\n",
        "    return min(contradiction_count / len(patterns), 1.0) if patterns else 0.0\n",
        "\n",
        "def pattern_based_hallucination_detection(response):\n",
        "    \"\"\"Fallback hallucination detection using patterns\"\"\"\n",
        "    score = 0.0\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    uncertainty_patterns = [\n",
        "        r\"\\b(I think|I believe|probably|maybe|perhaps|likely)\\b\",\n",
        "        r\"\\b(studies show|research indicates|experts say)\\b\",\n",
        "    ]\n",
        "\n",
        "    for pattern in uncertainty_patterns:\n",
        "        matches = re.findall(pattern, response_lower)\n",
        "        score += len(matches) * 0.1\n",
        "\n",
        "    overgeneralizations = re.findall(r\"\\b(always|never|every|all|none)\\b\", response_lower)\n",
        "    score += len(overgeneralizations) * 0.15\n",
        "\n",
        "    sensational_claims = re.findall(r\"\\b(cure|miracle|breakthrough|revolutionary)\\b\", response_lower)\n",
        "    score += len(sensational_claims) * 0.2\n",
        "\n",
        "    return min(score, 1.0)\n",
        "\n",
        "def calculate_confidence(response):\n",
        "    \"\"\"Calculate confidence score based on response characteristics\"\"\"\n",
        "    confidence = 1.0\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    uncertainty_markers = [\"maybe\", \"perhaps\", \"I think\", \"I believe\", \"probably\"]\n",
        "    for marker in uncertainty_markers:\n",
        "        if marker in response_lower:\n",
        "            confidence -= 0.1\n",
        "\n",
        "    if len(response.split()) < 5:\n",
        "        confidence -= 0.2\n",
        "\n",
        "    return max(confidence, 0.1)\n",
        "\n",
        "def evaluate_hallucination(reference, response, category):\n",
        "    \"\"\"Evaluate hallucination using multiple methods\"\"\"\n",
        "    similarity_score = calculate_semantic_similarity(reference, response)\n",
        "\n",
        "    factual_score = check_factual_consistency(response, category)\n",
        "\n",
        "    pattern_score = pattern_based_hallucination_detection(response)\n",
        "\n",
        "    hallucination_score = 0.4 * (1 - similarity_score) + 0.4 * (1 - factual_score) + 0.2 * pattern_score\n",
        "\n",
        "    return min(hallucination_score, 1.0)\n",
        "\n",
        "def load_test_prompts(sample_count=3, dataset_name=\"all\"):\n",
        "    \"\"\"Load test prompts for evaluation from specified datasets - IMPROVED\"\"\"\n",
        "    prompts = []\n",
        "\n",
        "    if dataset_name == \"all\":\n",
        "        # Sample from ALL medical datasets\n",
        "        for dataset_key, dataset_prompts in MEDICAL_DATASETS.items():\n",
        "            samples_to_take = min(sample_count, len(dataset_prompts))\n",
        "            for i, prompt_data in enumerate(dataset_prompts[:samples_to_take]):\n",
        "                prompts.append({\n",
        "                    \"original_prompt\": prompt_data[\"question\"],\n",
        "                    \"clean_prompt\": prompt_data[\"question\"],\n",
        "                    \"original_reference\": prompt_data[\"reference\"],\n",
        "                    \"clean_reference\": prompt_data[\"reference\"],\n",
        "                    \"category\": prompt_data[\"category\"],\n",
        "                    \"dataset\": prompt_data[\"dataset\"]\n",
        "                })\n",
        "        # Also include some basic medical prompts\n",
        "        medical_samples = min(sample_count, len(MEDICAL_PROMPTS))\n",
        "        for i, prompt_data in enumerate(MEDICAL_PROMPTS[:medical_samples]):\n",
        "            prompts.append({\n",
        "                \"original_prompt\": prompt_data[\"question\"],\n",
        "                \"clean_prompt\": prompt_data[\"question\"],\n",
        "                \"original_reference\": prompt_data[\"reference\"],\n",
        "                \"clean_reference\": prompt_data[\"reference\"],\n",
        "                \"category\": prompt_data[\"category\"],\n",
        "                \"dataset\": \"medical_qa\"\n",
        "            })\n",
        "    elif dataset_name in MEDICAL_DATASETS:\n",
        "        # Sample from specific medical dataset\n",
        "        dataset_prompts = MEDICAL_DATASETS[dataset_name]\n",
        "        samples_to_take = min(sample_count, len(dataset_prompts))\n",
        "        for i, prompt_data in enumerate(dataset_prompts[:samples_to_take]):\n",
        "            prompts.append({\n",
        "                \"original_prompt\": prompt_data[\"question\"],\n",
        "                \"clean_prompt\": prompt_data[\"question\"],\n",
        "                \"original_reference\": prompt_data[\"reference\"],\n",
        "                \"clean_reference\": prompt_data[\"reference\"],\n",
        "                \"category\": prompt_data[\"category\"],\n",
        "                \"dataset\": prompt_data[\"dataset\"]\n",
        "            })\n",
        "    else:\n",
        "        # Fallback to original medical prompts\n",
        "        samples_to_take = min(sample_count, len(MEDICAL_PROMPTS))\n",
        "        for i, prompt_data in enumerate(MEDICAL_PROMPTS[:samples_to_take]):\n",
        "            prompts.append({\n",
        "                \"original_prompt\": prompt_data[\"question\"],\n",
        "                \"clean_prompt\": prompt_data[\"question\"],\n",
        "                \"original_reference\": prompt_data[\"reference\"],\n",
        "                \"clean_reference\": prompt_data[\"reference\"],\n",
        "                \"category\": prompt_data[\"category\"],\n",
        "                \"dataset\": \"medical_qa\"\n",
        "            })\n",
        "\n",
        "    return prompts\n",
        "\n",
        "def evaluate_model_responses(model, prompts, model_name, dataset=\"medical_qa\"):\n",
        "    \"\"\"Evaluate model responses for hallucinations with better error handling\"\"\"\n",
        "    results = {\n",
        "        \"model\": model_name,\n",
        "        \"dataset\": dataset,\n",
        "        \"sample_count\": len(prompts),\n",
        "        \"metrics\": {},\n",
        "        \"dataset_metrics\": {},\n",
        "        \"sample_responses\": []\n",
        "    }\n",
        "\n",
        "    hallucination_scores = []\n",
        "    accuracy_scores = []\n",
        "    confidence_scores = []\n",
        "\n",
        "    for prompt in tqdm(prompts, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            response = generate_response(model, prompt[\"clean_prompt\"], use_rag=ENABLE_RAG, model_name=model_name)\n",
        "\n",
        "            if response.startswith(\"Error:\"):\n",
        "                accuracy = 0.0\n",
        "                hallucination_score = 0.5\n",
        "                confidence = 0.1\n",
        "            else:\n",
        "                accuracy = calculate_semantic_similarity(prompt[\"clean_reference\"], response)\n",
        "                hallucination_score = evaluate_hallucination(prompt[\"clean_reference\"], response, prompt[\"category\"])\n",
        "                confidence = calculate_confidence(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating prompt: {str(e)}\")\n",
        "            response = f\"Error: {str(e)}\"\n",
        "            accuracy = 0.0\n",
        "            hallucination_score = 0.5\n",
        "            confidence = 0.1\n",
        "\n",
        "        accuracy_scores.append(accuracy)\n",
        "        hallucination_scores.append(hallucination_score)\n",
        "        confidence_scores.append(confidence)\n",
        "\n",
        "        results[\"sample_responses\"].append({\n",
        "            \"prompt\": prompt[\"original_prompt\"],\n",
        "            \"reference\": prompt[\"original_reference\"],\n",
        "            \"response\": response,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"hallucination_score\": hallucination_score,\n",
        "            \"confidence\": confidence\n",
        "        })\n",
        "\n",
        "    if accuracy_scores and hallucination_scores:\n",
        "        results[\"metrics\"] = {\n",
        "            \"accuracy\": np.mean(accuracy_scores),\n",
        "            \"hallucination_rate\": np.mean(hallucination_scores),\n",
        "            \"confidence\": np.mean(confidence_scores),\n",
        "            \"response_length\": np.mean([len(str(r[\"response\"])) for r in results[\"sample_responses\"]]),\n",
        "            \"consistency\": 1.0 - np.std(hallucination_scores) if len(hallucination_scores) > 1 else 1.0\n",
        "        }\n",
        "\n",
        "        # Calculate metrics by dataset\n",
        "        dataset_metrics = {}\n",
        "        for prompt in prompts:\n",
        "            dataset_name = prompt.get(\"dataset\", \"unknown\")\n",
        "            if dataset_name not in dataset_metrics:\n",
        "                dataset_metrics[dataset_name] = {\n",
        "                    \"accuracy_scores\": [],\n",
        "                    \"hallucination_scores\": [],\n",
        "                    \"confidence_scores\": []\n",
        "                }\n",
        "\n",
        "        for i, response_data in enumerate(results[\"sample_responses\"]):\n",
        "            dataset_name = prompts[i].get(\"dataset\", \"unknown\")\n",
        "            dataset_metrics[dataset_name][\"accuracy_scores\"].append(response_data[\"accuracy\"])\n",
        "            dataset_metrics[dataset_name][\"hallucination_scores\"].append(response_data[\"hallucination_score\"])\n",
        "            dataset_metrics[dataset_name][\"confidence_scores\"].append(response_data[\"confidence\"])\n",
        "\n",
        "        # Compute average metrics for each dataset\n",
        "        for dataset_name, metrics in dataset_metrics.items():\n",
        "            results[\"dataset_metrics\"][dataset_name] = {\n",
        "                \"accuracy\": np.mean(metrics[\"accuracy_scores\"]) if metrics[\"accuracy_scores\"] else 0,\n",
        "                \"hallucination_rate\": np.mean(metrics[\"hallucination_scores\"]) if metrics[\"hallucination_scores\"] else 0,\n",
        "                \"confidence\": np.mean(metrics[\"confidence_scores\"]) if metrics[\"confidence_scores\"] else 0,\n",
        "                \"sample_count\": len(metrics[\"accuracy_scores\"])\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "def generate_improvement_suggestions(metrics):\n",
        "    \"\"\"Generate suggestions based on evaluation results\"\"\"\n",
        "    suggestions = []\n",
        "\n",
        "    hallucination_rate = metrics.get(\"hallucination_rate\", 0)\n",
        "    accuracy = metrics.get(\"accuracy\", 0)\n",
        "    confidence = metrics.get(\"confidence\", 0)\n",
        "    consistency = metrics.get(\"consistency\", 0)\n",
        "\n",
        "    if hallucination_rate > 0.3:\n",
        "        suggestions.append({\n",
        "            \"category\": \"High Priority\",\n",
        "            \"suggestion\": \"Implement RAG with verified medical knowledge base\",\n",
        "            \"expected_impact\": \"40-60% reduction in hallucinations\"\n",
        "        })\n",
        "\n",
        "    if accuracy < 0.6:\n",
        "        suggestions.append({\n",
        "            \"category\": \"High Priority\",\n",
        "            \"suggestion\": \"Fine-tune with curated medical QA pairs and implement fact-checking\",\n",
        "            \"expected_impact\": \"30-50% accuracy improvement\"\n",
        "        })\n",
        "\n",
        "    if confidence < 0.6:\n",
        "        suggestions.append({\n",
        "            \"category\": \"Medium Priority\",\n",
        "            \"suggestion\": \"Add confidence calibration and uncertainty quantification\",\n",
        "            \"expected_impact\": \"Better reliability estimation and fewer overconfident errors\"\n",
        "        })\n",
        "\n",
        "    if consistency < 0.7:\n",
        "        suggestions.append({\n",
        "            \"category\": \"Medium Priority\",\n",
        "            \"suggestion\": \"Implement response consistency checks and self-verification\",\n",
        "            \"expected_impact\": \"More consistent and reliable responses\"\n",
        "        })\n",
        "\n",
        "    suggestions.append({\n",
        "        \"category\": \"General\",\n",
        "        \"suggestion\": \"Implement multi-step verification: claim extraction → fact checking → response generation\",\n",
        "        \"expected_impact\": \"Overall quality and reliability improvement\"\n",
        "    })\n",
        "\n",
        "    return suggestions"
      ],
      "metadata": {
        "id": "f7JWg7blCLkS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation Functions"
      ],
      "metadata": {
        "id": "b5oL-nEcCVvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_local_model(model_name, sample_count=3):\n",
        "    \"\"\"Evaluate a local model\"\"\"\n",
        "    print(f\"Evaluating local model: {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        model_pipeline = load_local_model(model_name)\n",
        "\n",
        "        prompts = load_test_prompts(sample_count)\n",
        "\n",
        "        results = evaluate_model_responses(model_pipeline, prompts, model_name)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"error\": str(e),\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": 0,\n",
        "                \"hallucination_rate\": 1.0,\n",
        "                \"confidence\": 0,\n",
        "                \"response_length\": 0,\n",
        "                \"consistency\": 0\n",
        "            },\n",
        "            \"evaluation_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "def evaluate_single_model(model_name, sample_count=3):\n",
        "    \"\"\"Evaluate a single local model\"\"\"\n",
        "    results = evaluate_local_model(model_name, sample_count)\n",
        "\n",
        "    # NEW: Evaluate on ALL medical datasets\n",
        "    dataset_results = {}\n",
        "    for dataset_name in [\"pubmedqa\", \"medqa\", \"mimic_cxr\"]:\n",
        "        try:\n",
        "            dataset_eval = evaluate_model_on_dataset(model_name, dataset_name, sample_count)\n",
        "            dataset_results[dataset_name] = dataset_eval\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating on {dataset_name}: {str(e)}\")\n",
        "            dataset_results[dataset_name] = {\n",
        "                \"error\": str(e),\n",
        "                \"dataset\": dataset_name\n",
        "            }\n",
        "\n",
        "    baseline = {\n",
        "        \"accuracy\": 0.7,\n",
        "        \"hallucination_rate\": 0.25,\n",
        "        \"fact_score\": 0.75\n",
        "    }\n",
        "\n",
        "    if \"error\" not in results:\n",
        "        hallucination_reduction = (\n",
        "            ((baseline[\"hallucination_rate\"] - results[\"metrics\"][\"hallucination_rate\"]) / baseline[\"hallucination_rate\"] * 100)\n",
        "            if baseline[\"hallucination_rate\"] > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        accuracy_improvement = (\n",
        "            ((results[\"metrics\"][\"accuracy\"] - baseline[\"accuracy\"]) / baseline[\"accuracy\"] * 100)\n",
        "            if baseline[\"accuracy\"] > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        response_data = {\n",
        "            \"model\": model_name,\n",
        "            \"dataset\": \"combined_medical\",  # Changed from \"medical_qa\"\n",
        "            \"sample_count\": sample_count,\n",
        "            \"metrics\": results[\"metrics\"],\n",
        "            \"dataset_metrics\": dataset_results,  # NEW: Include all dataset results\n",
        "            \"baseline\": baseline,\n",
        "            \"improvement\": {\n",
        "                \"hallucination_reduction\": hallucination_reduction,\n",
        "                \"accuracy_improvement\": accuracy_improvement,\n",
        "            },\n",
        "            \"suggestions\": generate_improvement_suggestions(results[\"metrics\"]),\n",
        "            \"sample_responses\": results.get(\"sample_responses\", [])[:3],\n",
        "            \"evaluation_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return response_data\n",
        "    else:\n",
        "        return results\n",
        "\n",
        "def evaluate_all_models(sample_count=3):\n",
        "    \"\"\"Evaluate all available models\"\"\"\n",
        "    model_configs = get_model_configs()\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name in model_configs.keys():\n",
        "        results = evaluate_single_model(model_name, sample_count)\n",
        "        all_results[model_name] = results\n",
        "\n",
        "        with open(f\"{model_name}_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results for {model_name} saved to {model_name}_results.json\")\n",
        "\n",
        "    with open(\"all_model_results.json\", \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(\"All results saved to all_model_results.json\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def evaluate_models_by_category(category, sample_count=3):\n",
        "    \"\"\"Evaluate models by category\"\"\"\n",
        "    categories = get_model_categories()\n",
        "\n",
        "    if category not in categories:\n",
        "        raise ValueError(f\"Unknown category: {category}\")\n",
        "\n",
        "    model_names = categories[category]\n",
        "    results = {}\n",
        "\n",
        "    for model_name in model_names:\n",
        "        if category == \"local_models\":\n",
        "            results[model_name] = evaluate_local_model(model_name, sample_count)\n",
        "        else:\n",
        "            results[model_name] = evaluate_cloud_model(model_name, sample_count)\n",
        "\n",
        "    with open(f\"{category}_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"Results for {category} saved to {category}_results.json\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_model_on_dataset(model_name, dataset_name, sample_count=3):\n",
        "    \"\"\"Evaluate a model on a specific dataset\"\"\"\n",
        "    print(f\"Evaluating {model_name} on {dataset_name} dataset...\")\n",
        "    try:\n",
        "        # Load local model\n",
        "        model = load_local_model(model_name)\n",
        "\n",
        "        # Load test prompts for specific dataset\n",
        "        prompts = load_test_prompts(sample_count, dataset_name)\n",
        "\n",
        "        # Evaluate model\n",
        "        results = evaluate_model_responses(model, prompts, model_name, dataset_name)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name} on {dataset_name}: {str(e)}\")\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"dataset\": dataset_name,\n",
        "            \"error\": str(e),\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": 0,\n",
        "                \"hallucination_rate\": 1.0,\n",
        "                \"confidence\": 0,\n",
        "                \"response_length\": 0,\n",
        "                \"consistency\": 0\n",
        "            },\n",
        "            \"evaluation_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "def evaluate_all_models_on_datasets(sample_count=3):\n",
        "    \"\"\"Evaluate all models on all datasets\"\"\"\n",
        "    model_configs = get_model_configs()\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name in model_configs.keys():\n",
        "        model_results = {}\n",
        "\n",
        "        for dataset_name in [\"pubmedqa\", \"medqa\", \"mimic_cxr\", \"medical_qa\"]:\n",
        "            results = evaluate_model_on_dataset(model_name, dataset_name, sample_count)\n",
        "            model_results[dataset_name] = results\n",
        "\n",
        "        # Also evaluate on all datasets combined\n",
        "        combined_results = evaluate_single_model(model_name, sample_count)\n",
        "        model_results[\"combined\"] = combined_results\n",
        "\n",
        "        all_results[model_name] = model_results\n",
        "\n",
        "        # Save individual model results\n",
        "        with open(f\"{model_name}_dataset_results.json\", \"w\") as f:\n",
        "            json.dump(model_results, f, indent=2)\n",
        "        print(f\"Results for {model_name} saved to {model_name}_dataset_results.json\")\n",
        "\n",
        "    # Save all results together\n",
        "    with open(\"all_models_dataset_results.json\", \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(\"All dataset results saved to all_models_dataset_results.json\")\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "2JVN0x4aCTjw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Functions"
      ],
      "metadata": {
        "id": "iamMaWn2Cbln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bar_chart_base64(model_results, metric=\"accuracy\"):\n",
        "    \"\"\"Create bar chart and return as base64 string\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v}\n",
        "\n",
        "    if not valid_models:\n",
        "        return None\n",
        "\n",
        "    models = list(valid_models.keys())\n",
        "    values = [valid_models[model][\"metrics\"].get(metric, 0) for model in models]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bars = ax.bar(models, values, color=['#4CAF50', '#2196F3', '#FF9800', '#E91E63', '#9C27B0'])\n",
        "\n",
        "    # Customize the chart\n",
        "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
        "    ax.set_title(f'Model Comparison - {metric.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(values):\n",
        "        ax.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to base64\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_base64"
      ],
      "metadata": {
        "id": "1exDfJ11CavL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate data, visualization"
      ],
      "metadata": {
        "id": "JtqUgfDsIZz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_ui_export_data(model_results, model_name):\n",
        "    \"\"\"Create the specialized UI export data that includes medical datasets\"\"\"\n",
        "    if \"error\" in model_results:\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"error\": model_results[\"error\"],\n",
        "            \"evaluation_date\": model_results.get(\"evaluation_date\", datetime.now().isoformat())\n",
        "        }\n",
        "\n",
        "    # Extract sample responses for UI display\n",
        "    sample_responses = []\n",
        "    for i, response_data in enumerate(model_results.get(\"sample_responses\", [])[:5]):\n",
        "        sample_responses.append({\n",
        "            \"id\": i + 1,\n",
        "            \"prompt\": response_data[\"prompt\"],\n",
        "            \"reference\": response_data[\"reference\"],\n",
        "            \"response\": response_data[\"response\"],\n",
        "            \"accuracy\": response_data[\"accuracy\"],\n",
        "            \"hallucination_score\": response_data[\"hallucination_score\"],\n",
        "            \"dataset\": response_data.get(\"dataset\", \"medical_qa\")  # NEW: Include dataset info\n",
        "        })\n",
        "\n",
        "    # Extract dataset metrics for UI\n",
        "    dataset_performance = {}\n",
        "    for dataset_name, dataset_data in model_results.get(\"dataset_metrics\", {}).items():\n",
        "        if \"metrics\" in dataset_data:\n",
        "            dataset_performance[dataset_name] = dataset_data[\"metrics\"]\n",
        "\n",
        "    # Create the UI export data structure\n",
        "    ui_export_data = {\n",
        "        \"model\": model_name,\n",
        "        \"evaluation_date\": model_results.get(\"evaluation_date\", datetime.now().isoformat()),\n",
        "        \"metrics\": model_results.get(\"metrics\", {}),\n",
        "        \"dataset_metrics\": dataset_performance,  # NEW: Include dataset performance\n",
        "        \"dataset_details\": model_results.get(\"dataset_metrics\", {}),  # NEW: Full dataset details\n",
        "        \"sample_responses\": sample_responses,\n",
        "        \"suggestions\": model_results.get(\"suggestions\", []),\n",
        "        \"improvement\": model_results.get(\"improvement\", {}),\n",
        "        \"baseline\": model_results.get(\"baseline\", {})\n",
        "    }\n",
        "\n",
        "    return ui_export_data\n",
        "\n",
        "\n",
        "def create_visualization_directory_structure(model_name):\n",
        "    \"\"\"Create directory structure for storing visualization files - FIXED STRUCTURE\"\"\"\n",
        "    # Create a unified directory structure\n",
        "    base_dir = f\"model_evaluation_{model_name}\"\n",
        "\n",
        "    sub_dirs = {\n",
        "        'charts': f\"{base_dir}/charts\",\n",
        "        'data': f\"{base_dir}/data\",\n",
        "        'tables': f\"{base_dir}/tables\",\n",
        "        'dataset_analysis': f\"{base_dir}/dataset_analysis\"\n",
        "    }\n",
        "\n",
        "    # Create all directories\n",
        "    for dir_path in sub_dirs.values():\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        print(f\"Created directory: {dir_path}\")\n",
        "\n",
        "    return base_dir, sub_dirs\n",
        "\n",
        "def export_visualizations_to_directories(model_results, model_name):\n",
        "    \"\"\"Export all visualizations to organized directory structure - FIXED VERSION\"\"\"\n",
        "    try:\n",
        "        # Handle different input types\n",
        "        if isinstance(model_results, dict) and model_name in model_results:\n",
        "            model_data = model_results[model_name]\n",
        "        elif isinstance(model_results, dict) and len(model_results) == 1:\n",
        "            model_data = next(iter(model_results.values()))\n",
        "        else:\n",
        "            model_data = model_results\n",
        "\n",
        "        # Check if we have valid data\n",
        "        if not isinstance(model_data, dict) or \"error\" in model_data:\n",
        "            print(f\"Error: Invalid model data for {model_name}\")\n",
        "            return None, []\n",
        "\n",
        "        base_dir, sub_dirs = create_visualization_directory_structure(model_name)\n",
        "        exported_files = []\n",
        "\n",
        "        # Create and save UI export data\n",
        "        ui_export_data = create_ui_export_data(model_data, model_name)\n",
        "        ui_export_filename = f\"{sub_dirs['data']}/{model_name}_ui_export_data.json\"\n",
        "        with open(ui_export_filename, \"w\") as f:\n",
        "            json.dump(ui_export_data, f, indent=2)\n",
        "        exported_files.append(ui_export_filename)\n",
        "\n",
        "        # Create and save comprehensive results\n",
        "        comprehensive_filename = f\"{sub_dirs['data']}/{model_name}_comprehensive_results.json\"\n",
        "        with open(comprehensive_filename, \"w\") as f:\n",
        "            json.dump(model_data, f, indent=2)\n",
        "        exported_files.append(comprehensive_filename)\n",
        "\n",
        "        # For single model, create a dict format for visualization functions\n",
        "        model_results_dict = {model_name: model_data}\n",
        "\n",
        "        # Bar charts for key metrics\n",
        "        for metric in [\"accuracy\", \"hallucination_rate\", \"confidence\"]:\n",
        "            img_data = create_bar_chart_base64(model_results_dict, metric)\n",
        "            if img_data:\n",
        "                filename = f\"{sub_dirs['charts']}/{metric}_bar_chart.png\"\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(base64.b64decode(img_data))\n",
        "                exported_files.append(filename)\n",
        "                print(f\"Created chart: {filename}\")\n",
        "\n",
        "        # Radar chart\n",
        "        radar_img = create_radar_chart_base64(model_results_dict)\n",
        "        if radar_img:\n",
        "            filename = f\"{sub_dirs['charts']}/radar_chart.png\"\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(base64.b64decode(radar_img))\n",
        "            exported_files.append(filename)\n",
        "            print(f\"Created chart: {filename}\")\n",
        "\n",
        "        # Comparison table\n",
        "        comparison_table = create_comparison_table(model_results_dict)\n",
        "        if not comparison_table.empty:\n",
        "            html_filename = f\"{sub_dirs['tables']}/comparison_table.html\"\n",
        "            with open(html_filename, \"w\") as f:\n",
        "                f.write(comparison_table.to_html(classes='table table-striped', index=False))\n",
        "            exported_files.append(html_filename)\n",
        "            print(f\"Created table: {html_filename}\")\n",
        "\n",
        "        # Additional visualizations for medical datasets if available\n",
        "        if \"dataset_metrics\" in model_data and model_data[\"dataset_metrics\"]:\n",
        "            # Dataset comparison chart\n",
        "            dataset_chart = create_dataset_comparison_chart(model_results_dict)\n",
        "            if dataset_chart:\n",
        "                filename = f\"{sub_dirs['charts']}/dataset_comparison_chart.png\"\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(base64.b64decode(dataset_chart))\n",
        "                exported_files.append(filename)\n",
        "                print(f\"Created chart: {filename}\")\n",
        "\n",
        "            # Individual dataset radar charts\n",
        "            for dataset_name in [\"pubmedqa\", \"medqa\", \"mimic_cxr\"]:\n",
        "                if dataset_name in model_data.get(\"dataset_metrics\", {}):\n",
        "                    dataset_radar = create_dataset_radar_chart(model_results_dict, dataset_name)\n",
        "                    if dataset_radar:\n",
        "                        filename = f\"{sub_dirs['charts']}/{dataset_name}_radar_chart.png\"\n",
        "                        with open(filename, \"wb\") as f:\n",
        "                            f.write(base64.b64decode(dataset_radar))\n",
        "                        exported_files.append(filename)\n",
        "                        print(f\"Created chart: {filename}\")\n",
        "\n",
        "        print(f\"Exported {len(exported_files)} files to {base_dir}/ directory structure\")\n",
        "        return base_dir, exported_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in export_visualizations_to_directories: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, []\n",
        "\n",
        "def create_zip_from_directory(base_dir):\n",
        "    \"\"\"Create ZIP archive from directory structure - SIMPLIFIED\"\"\"\n",
        "    zip_filename = f\"{base_dir}.zip\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(base_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Add file to zip with relative path\n",
        "                arcname = os.path.relpath(file_path, base_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"Created ZIP archive: {zip_filename}\")\n",
        "    return zip_filename\n",
        "\n",
        "def create_radar_chart_base64(model_results):\n",
        "    \"\"\"Create radar chart comparing multiple metrics across models - UPDATED for single model\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v}\n",
        "\n",
        "    # Handle single model case by creating a minimal comparison\n",
        "    if len(valid_models) == 1:\n",
        "        # For single model, create a radar chart with just that model\n",
        "        model_name, results = next(iter(valid_models.items()))\n",
        "        metrics = ['accuracy', 'confidence', 'consistency']\n",
        "        labels = ['Accuracy', 'Confidence', 'Consistency']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "        values = [results[\"metrics\"].get(metric, 0) for metric in metrics]\n",
        "        values += values[:1]  # Close the radar chart\n",
        "\n",
        "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "        angles += angles[:1]\n",
        "\n",
        "        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color='#4CAF50')\n",
        "        ax.fill(angles, values, alpha=0.1, color='#4CAF50')\n",
        "\n",
        "        ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(f'{model_name} Performance Radar Chart', size=14, fontweight='bold')\n",
        "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert to base64\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        return img_base64\n",
        "\n",
        "    elif len(valid_models) >= 2:\n",
        "        # Original multi-model code\n",
        "        metrics = ['accuracy', 'confidence', 'consistency']\n",
        "        labels = ['Accuracy', 'Confidence', 'Consistency']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
        "\n",
        "        colors = ['#4CAF50', '#2196F3', '#FF9800', '#E91E63', '#9C27B0']\n",
        "\n",
        "        for i, (model_name, results) in enumerate(valid_models.items()):\n",
        "            values = [results[\"metrics\"].get(metric, 0) for metric in metrics]\n",
        "            values += values[:1]  # Close the radar chart\n",
        "\n",
        "            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "            angles += angles[:1]\n",
        "\n",
        "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])\n",
        "            ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
        "\n",
        "        ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title('Model Performance Radar Chart', size=14, fontweight='bold')\n",
        "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert to base64\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        return img_base64\n",
        "\n",
        "    return None\n",
        "\n",
        "def create_dataset_comparison_chart(model_results):\n",
        "    \"\"\"Create chart comparing performance across medical datasets - UPDATED\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v and \"dataset_metrics\" in v}\n",
        "\n",
        "    if not valid_models:\n",
        "        return None\n",
        "\n",
        "    # Get all medical dataset names\n",
        "    dataset_names = [\"pubmedqa\", \"medqa\", \"mimic_cxr\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Accuracy by dataset\n",
        "    for model_name, results in valid_models.items():\n",
        "        accuracies = []\n",
        "        for dataset in dataset_names:\n",
        "            if dataset in results.get(\"dataset_metrics\", {}):\n",
        "                accuracies.append(results[\"dataset_metrics\"][dataset].get(\"metrics\", {}).get(\"accuracy\", 0))\n",
        "            else:\n",
        "                accuracies.append(0)\n",
        "\n",
        "        axes[0].plot(dataset_names, accuracies, 'o-', label=model_name, linewidth=2, markersize=8)\n",
        "\n",
        "    axes[0].set_title('Accuracy Across Medical Datasets', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_ylim(0, 1)\n",
        "\n",
        "    # Hallucination rate by dataset\n",
        "    for model_name, results in valid_models.items():\n",
        "        hall_rates = []\n",
        "        for dataset in dataset_names:\n",
        "            if dataset in results.get(\"dataset_metrics\", {}):\n",
        "                hall_rates.append(results[\"dataset_metrics\"][dataset].get(\"metrics\", {}).get(\"hallucination_rate\", 0))\n",
        "            else:\n",
        "                hall_rates.append(0)\n",
        "\n",
        "        axes[1].plot(dataset_names, hall_rates, 'o-', label=model_name, linewidth=2, markersize=8)\n",
        "\n",
        "    axes[1].set_title('Hallucination Rate Across Medical Datasets', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Hallucination Rate')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to base64\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_base64\n",
        "\n",
        "\n",
        "def create_dataset_radar_chart(model_results, dataset_name):\n",
        "    \"\"\"Create radar chart for a specific dataset\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v and \"dataset_metrics\" in v}\n",
        "\n",
        "    if not valid_models or dataset_name not in next(iter(valid_models.values()))[\"dataset_metrics\"]:\n",
        "        return None\n",
        "\n",
        "    metrics = ['accuracy', 'confidence']\n",
        "    labels = ['Accuracy', 'Confidence']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "    colors = ['#4CAF50', '#2196F3', '#FF9800', '#E91E63', '#9C27B0']\n",
        "\n",
        "    for i, (model_name, results) in enumerate(valid_models.items()):\n",
        "        if dataset_name in results[\"dataset_metrics\"]:\n",
        "            values = [results[\"dataset_metrics\"][dataset_name].get(metric, 0) for metric in metrics]\n",
        "            values += values[:1]  # Close the radar chart\n",
        "\n",
        "            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "            angles += angles[:1]\n",
        "\n",
        "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])\n",
        "            ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
        "\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(f'Performance on {dataset_name.upper()} Dataset', size=14, fontweight='bold')\n",
        "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to base64\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_base64\n",
        "\n",
        "def create_comparison_table(model_results):\n",
        "    \"\"\"Create comprehensive comparison table\"\"\"\n",
        "    data = []\n",
        "    for model_name, results in model_results.items():\n",
        "        if \"error\" not in results:\n",
        "            data.append({\n",
        "                \"Model\": model_name,\n",
        "                \"Accuracy\": f\"{results['metrics'].get('accuracy', 0):.3f}\",\n",
        "                \"Hallucination Rate\": f\"{results['metrics'].get('hallucination_rate', 0):.3f}\",\n",
        "                \"Confidence\": f\"{results['metrics'].get('confidence', 0):.3f}\",\n",
        "                \"Response Length\": f\"{results['metrics'].get('response_length', 0):.1f}\",\n",
        "                \"Consistency\": f\"{results['metrics'].get('consistency', 0):.3f}\",\n",
        "                \"Sample Count\": results.get('sample_count', 0)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "e3VyQKevIWac"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "sample_count = 2\n",
        "def clear_gpu_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# Call this before evaluating each model\n",
        "clear_gpu_cache()\n",
        "# all_results = evaluate_all_models(sample_count=3)"
      ],
      "metadata": {
        "id": "_7-PNvNUiQt5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For single model export [\"llama-2-7b\", \"mistral-7b\", \"qwen-7b\", \"meditron-7b\", \"biomedgpt\", \"grok-2\", \"claude-3.7-sonnet\", \"gpt-oss-20b\"]\n",
        "model_name = \"gpt-oss-20b\"\n",
        "model_result = evaluate_single_model(model_name)\n",
        "\n",
        "base_dir, exported_files = export_visualizations_to_directories(\n",
        "    model_result, model_name\n",
        ")\n",
        "\n",
        "# Print what files were created\n",
        "print(\"Files created:\")\n",
        "for file in exported_files:\n",
        "    print(f\"  - {file}\")\n",
        "\n",
        "zip_filename = create_zip_from_directory(base_dir)\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779,
          "referenced_widgets": [
            "75b3c6a63db546c4b4b7ccfa4d020a58",
            "ea11e2362452477b93f8a6c856ed552f",
            "b62fe6e31cdd4c2887962dd9b9025c35",
            "dda28036b7994cef9ad5e448e30d17b9",
            "250cbfa4256d4f5aaac470439da07fed",
            "fb2c50333df6452287b3c85f9121ffad",
            "18b8b50c349d4e3d9017c3913dba2cf0",
            "cb54129f40274e40b9c6bb86507e4c63",
            "1d0a1381984540aab5d070abe547ac53",
            "0abed41f5a6e4ac29cc3fbed4205c7da",
            "836aec09560442d9883208e6cc643c31",
            "0d7c301df99348f8a6ca7baf9efd3d9d",
            "579d9cee91e84260bd79291d3628fde7",
            "97c0c68a12264869858cb2d613f3cb60",
            "763f4a58a3ad4430b431eb53f2c583fc",
            "e70fc661fc874140999c2b238f9457c8",
            "471d929fba1a4632a17c514908cbcd1e",
            "a67f80db9f2648acb728da0a3e06acc6",
            "7757b0cd559f401c8b90c53098ab9305",
            "f0a7b89054374f55a755a785907edc12",
            "7a047a48da214760a537165d56c0062c",
            "d30c1f7e219743dca3fd12150042e906",
            "d86abb9d07a040839164c70bd550a81f",
            "0ea5b1883994426f8f19728a01e06af5",
            "da86c87abe5242a6a132fc6298d5305f",
            "5d47ed735fb54b34a45dfbb21d4be668",
            "f7dd3f983b834aa2913b6aaf956df330",
            "dc522c6bb8f54d3489fd4d985815940c",
            "5df8e54a9bfd4232a615d27f674b5556",
            "a7e658b5cf8d4da49d196c58c15990f6",
            "840705e3aa11488099c61d6b536719c7",
            "e143bad7f1af487f98a0d48938d8356c",
            "99c9032070a3480ba1ee292a9a085c9f",
            "935283d14a794281a3b979b92feaa20a",
            "1b86852d678c4264982b6df2b58c26a8",
            "890015986aeb44a2a84dcb3235da0114",
            "ad684add5b754513b7b6e49f06353ac5",
            "a3692debf1f34c6b96faa0cd5d1867d1",
            "784c9e9be8fc44aba9533577899e6285",
            "ed82ec6d761b4632a313511df65d551a",
            "bd7af1ab46d4469ab27a14abb5c1ff7a",
            "e93b89d10fb540679eb6725d1d7ebab5",
            "e4c2f6bba1614c778f007d0f5a61a89a",
            "006d05f17b8549aab3e126218c1a0c9a",
            "505e0692fd0d4294b518764c1afd8d89",
            "d637833af53f490bb9918e8d0f989fe9",
            "b6deaecce1204c35a3f424e4bcd4f1c0",
            "c95ef00c54ba4806a9c563e8dcdbdddf",
            "f5144988cc2444ebb62beaf04dd5c7f5",
            "8285f9a9ccc2494f9335a94d9976f0ff",
            "d16a57204c244f9e93492128be903ea2",
            "4cdf7470677e45a6884a435557b9baad",
            "3e752f9b0c7c41a897eb2584f21bb891",
            "36beface9094427687474cbf7367c06f",
            "6cbe071ee292406387857b201ae99288"
          ]
        },
        "id": "yXncwdgQIuNb",
        "outputId": "8a14afa4-41e4-45cc-dfdd-564af6438dac"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating local model: gpt-oss-20b...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75b3c6a63db546c4b4b7ccfa4d020a58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d7c301df99348f8a6ca7baf9efd3d9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d86abb9d07a040839164c70bd550a81f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "935283d14a794281a3b979b92feaa20a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "505e0692fd0d4294b518764c1afd8d89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading gpt-oss-20b: The model is quantized with Mxfp4Config but you are passing a BitsAndBytesConfig config. Please make sure to pass the same quantization config class to `from_pretrained` with different loading attributes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating gpt-oss-20b:  11%|█         | 1/9 [00:03<00:26,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation error: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n",
            "Alternative generation also failed: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating gpt-oss-20b:  22%|██▏       | 2/9 [00:06<00:22,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation error: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n",
            "Alternative generation also failed: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating gpt-oss-20b:  33%|███▎      | 3/9 [00:09<00:19,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation error: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n",
            "Alternative generation also failed: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating gpt-oss-20b:  44%|████▍     | 4/9 [00:12<00:16,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation error: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n",
            "Alternative generation also failed: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating gpt-oss-20b:  56%|█████▌    | 5/9 [00:16<00:13,  3.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation error: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n",
            "Alternative generation also failed: cannot access free variable 'e' where it is not associated with a value in enclosing scope\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating gpt-oss-20b:  56%|█████▌    | 5/9 [00:17<00:14,  3.50s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1743396817.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For single model export [\"llama-2-7b\", \"mistral-7b\", \"qwen-7b\", \"meditron-7b\", \"biomedgpt\", \"grok-2\", \"claude-3.7-sonnet\", \"gpt-oss-20b\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-oss-20b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_single_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m base_dir, exported_files = export_visualizations_to_directories(\n",
            "\u001b[0;32m/tmp/ipython-input-3867163184.py\u001b[0m in \u001b[0;36mevaluate_single_model\u001b[0;34m(model_name, sample_count)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_single_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"\"\"Evaluate a single local model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_local_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# NEW: Evaluate on ALL medical datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3867163184.py\u001b[0m in \u001b[0;36mevaluate_local_model\u001b[0;34m(model_name, sample_count)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_test_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1173391822.py\u001b[0m in \u001b[0;36mevaluate_model_responses\u001b[0;34m(model, prompts, model_name, dataset)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Evaluating {model_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clean_prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mENABLE_RAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1656641101.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(model, prompt, use_rag, model_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m\"\"\"Generate response using local model only\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_response_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_rag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1656641101.py\u001b[0m in \u001b[0;36mgenerate_response_local\u001b[0;34m(model, prompt, use_rag)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_rag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_medical_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAG_TOP_K\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1709587152.py\u001b[0m in \u001b[0;36mcreate_medical_retriever\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedical_knowledge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     embeddings = HuggingFaceEmbeddings(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentence-transformers/all-mpnet-base-v2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_huggingface/embeddings/huggingface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmodel_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         self._client = model_cls(\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             ):\n\u001b[0;32m--> 327\u001b[0;31m                 modules, self.module_kwargs = self._load_sbert_model(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 \u001b[0;31m# Newer modules that support the new loading method are loaded with the new style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2252\u001b[0m                 \u001b[0;31m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2253\u001b[0;31m                 module = module_class.load(\n\u001b[0m\u001b[1;32m   2254\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2255\u001b[0m                     \u001b[0;31m# Loading-specific keyword arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     ) -> Self:\n\u001b[0;32m--> 325\u001b[0;31m         init_kwargs = cls._load_init_kwargs(\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m_load_init_kwargs\u001b[0;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> dict[str, Any]:\n\u001b[0;32m--> 358\u001b[0;31m         config = cls.load_config(\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, model_name_or_path, subfolder, config_filename, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m    421\u001b[0m         )\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconfig_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_filenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             config = super().load_config(\n\u001b[0m\u001b[1;32m    424\u001b[0m                 \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/models/Module.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(cls, model_name_or_path, subfolder, config_filename, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconfiguration\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         config_path = load_file_path(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_filename\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/util/file_io.py\u001b[0m in \u001b[0;36mload_file_path\u001b[0;34m(model_name_or_path, filename, subfolder, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         return hf_hub_download(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         )\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;31m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;31m# If we can't, a HEAD request error is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m     (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = _get_metadata_or_catch_error(\n\u001b[0m\u001b[1;32m   1074\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# Recursively follow relative redirects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twD7IwpmxOOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOsJl2dtHlhnH0w1AytcmJp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75b3c6a63db546c4b4b7ccfa4d020a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea11e2362452477b93f8a6c856ed552f",
              "IPY_MODEL_b62fe6e31cdd4c2887962dd9b9025c35",
              "IPY_MODEL_dda28036b7994cef9ad5e448e30d17b9"
            ],
            "layout": "IPY_MODEL_250cbfa4256d4f5aaac470439da07fed"
          }
        },
        "ea11e2362452477b93f8a6c856ed552f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb2c50333df6452287b3c85f9121ffad",
            "placeholder": "​",
            "style": "IPY_MODEL_18b8b50c349d4e3d9017c3913dba2cf0",
            "value": "tokenizer_config.json: "
          }
        },
        "b62fe6e31cdd4c2887962dd9b9025c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb54129f40274e40b9c6bb86507e4c63",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d0a1381984540aab5d070abe547ac53",
            "value": 1
          }
        },
        "dda28036b7994cef9ad5e448e30d17b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0abed41f5a6e4ac29cc3fbed4205c7da",
            "placeholder": "​",
            "style": "IPY_MODEL_836aec09560442d9883208e6cc643c31",
            "value": " 4.20k/? [00:00&lt;00:00, 464kB/s]"
          }
        },
        "250cbfa4256d4f5aaac470439da07fed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb2c50333df6452287b3c85f9121ffad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18b8b50c349d4e3d9017c3913dba2cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb54129f40274e40b9c6bb86507e4c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1d0a1381984540aab5d070abe547ac53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0abed41f5a6e4ac29cc3fbed4205c7da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836aec09560442d9883208e6cc643c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d7c301df99348f8a6ca7baf9efd3d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_579d9cee91e84260bd79291d3628fde7",
              "IPY_MODEL_97c0c68a12264869858cb2d613f3cb60",
              "IPY_MODEL_763f4a58a3ad4430b431eb53f2c583fc"
            ],
            "layout": "IPY_MODEL_e70fc661fc874140999c2b238f9457c8"
          }
        },
        "579d9cee91e84260bd79291d3628fde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471d929fba1a4632a17c514908cbcd1e",
            "placeholder": "​",
            "style": "IPY_MODEL_a67f80db9f2648acb728da0a3e06acc6",
            "value": "tokenizer.json: 100%"
          }
        },
        "97c0c68a12264869858cb2d613f3cb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7757b0cd559f401c8b90c53098ab9305",
            "max": 27868174,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0a7b89054374f55a755a785907edc12",
            "value": 27868174
          }
        },
        "763f4a58a3ad4430b431eb53f2c583fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a047a48da214760a537165d56c0062c",
            "placeholder": "​",
            "style": "IPY_MODEL_d30c1f7e219743dca3fd12150042e906",
            "value": " 27.9M/27.9M [00:01&lt;00:00, 23.5MB/s]"
          }
        },
        "e70fc661fc874140999c2b238f9457c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471d929fba1a4632a17c514908cbcd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67f80db9f2648acb728da0a3e06acc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7757b0cd559f401c8b90c53098ab9305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0a7b89054374f55a755a785907edc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a047a48da214760a537165d56c0062c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d30c1f7e219743dca3fd12150042e906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d86abb9d07a040839164c70bd550a81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ea5b1883994426f8f19728a01e06af5",
              "IPY_MODEL_da86c87abe5242a6a132fc6298d5305f",
              "IPY_MODEL_5d47ed735fb54b34a45dfbb21d4be668"
            ],
            "layout": "IPY_MODEL_f7dd3f983b834aa2913b6aaf956df330"
          }
        },
        "0ea5b1883994426f8f19728a01e06af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc522c6bb8f54d3489fd4d985815940c",
            "placeholder": "​",
            "style": "IPY_MODEL_5df8e54a9bfd4232a615d27f674b5556",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "da86c87abe5242a6a132fc6298d5305f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7e658b5cf8d4da49d196c58c15990f6",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_840705e3aa11488099c61d6b536719c7",
            "value": 98
          }
        },
        "5d47ed735fb54b34a45dfbb21d4be668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e143bad7f1af487f98a0d48938d8356c",
            "placeholder": "​",
            "style": "IPY_MODEL_99c9032070a3480ba1ee292a9a085c9f",
            "value": " 98.0/98.0 [00:00&lt;00:00, 13.1kB/s]"
          }
        },
        "f7dd3f983b834aa2913b6aaf956df330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc522c6bb8f54d3489fd4d985815940c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5df8e54a9bfd4232a615d27f674b5556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7e658b5cf8d4da49d196c58c15990f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "840705e3aa11488099c61d6b536719c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e143bad7f1af487f98a0d48938d8356c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c9032070a3480ba1ee292a9a085c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "935283d14a794281a3b979b92feaa20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b86852d678c4264982b6df2b58c26a8",
              "IPY_MODEL_890015986aeb44a2a84dcb3235da0114",
              "IPY_MODEL_ad684add5b754513b7b6e49f06353ac5"
            ],
            "layout": "IPY_MODEL_a3692debf1f34c6b96faa0cd5d1867d1"
          }
        },
        "1b86852d678c4264982b6df2b58c26a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_784c9e9be8fc44aba9533577899e6285",
            "placeholder": "​",
            "style": "IPY_MODEL_ed82ec6d761b4632a313511df65d551a",
            "value": "chat_template.jinja: "
          }
        },
        "890015986aeb44a2a84dcb3235da0114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd7af1ab46d4469ab27a14abb5c1ff7a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e93b89d10fb540679eb6725d1d7ebab5",
            "value": 1
          }
        },
        "ad684add5b754513b7b6e49f06353ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4c2f6bba1614c778f007d0f5a61a89a",
            "placeholder": "​",
            "style": "IPY_MODEL_006d05f17b8549aab3e126218c1a0c9a",
            "value": " 16.7k/? [00:00&lt;00:00, 2.01MB/s]"
          }
        },
        "a3692debf1f34c6b96faa0cd5d1867d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "784c9e9be8fc44aba9533577899e6285": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed82ec6d761b4632a313511df65d551a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd7af1ab46d4469ab27a14abb5c1ff7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e93b89d10fb540679eb6725d1d7ebab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4c2f6bba1614c778f007d0f5a61a89a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "006d05f17b8549aab3e126218c1a0c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "505e0692fd0d4294b518764c1afd8d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d637833af53f490bb9918e8d0f989fe9",
              "IPY_MODEL_b6deaecce1204c35a3f424e4bcd4f1c0",
              "IPY_MODEL_c95ef00c54ba4806a9c563e8dcdbdddf"
            ],
            "layout": "IPY_MODEL_f5144988cc2444ebb62beaf04dd5c7f5"
          }
        },
        "d637833af53f490bb9918e8d0f989fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8285f9a9ccc2494f9335a94d9976f0ff",
            "placeholder": "​",
            "style": "IPY_MODEL_d16a57204c244f9e93492128be903ea2",
            "value": "config.json: "
          }
        },
        "b6deaecce1204c35a3f424e4bcd4f1c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cdf7470677e45a6884a435557b9baad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e752f9b0c7c41a897eb2584f21bb891",
            "value": 1
          }
        },
        "c95ef00c54ba4806a9c563e8dcdbdddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36beface9094427687474cbf7367c06f",
            "placeholder": "​",
            "style": "IPY_MODEL_6cbe071ee292406387857b201ae99288",
            "value": " 1.81k/? [00:00&lt;00:00, 197kB/s]"
          }
        },
        "f5144988cc2444ebb62beaf04dd5c7f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8285f9a9ccc2494f9335a94d9976f0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d16a57204c244f9e93492128be903ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cdf7470677e45a6884a435557b9baad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3e752f9b0c7c41a897eb2584f21bb891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36beface9094427687474cbf7367c06f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cbe071ee292406387857b201ae99288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}