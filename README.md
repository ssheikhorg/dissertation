# Evaluating and Mitigating Hallucinations in Large Language Models for Healthcare

## Project Overview
This project focuses on **evaluating and mitigating hallucinations** in **Large Language Models (LLMs)** for healthcare applications. Hallucinations refer to the generation of **plausible but factually incorrect** information by LLMs, which can lead to **misdiagnoses**, **incorrect treatment recommendations**, and **other healthcare risks**. The project aims to reduce hallucination rates and improve model accuracy in healthcare contexts.

### Key Features
- **Evaluation of multiple LLMs** including **BiomedGPT**, **GPT-3.5-Turbo**, **Medtrion-7B**, **Mistral-7B**, **Llama-2-7B**, and **Qwen-7B** on medical datasets.
- Use of **Retrieval-Augmented Generation (RAG)** and **LoRA fine-tuning** to **reduce hallucinations** and improve model reliability.
- **FastAPI** backend for model interaction, **AWS Lambda** for cloud deployment, and **HTML, CSS, JavaScript** for frontend display.
- **Metrics** for evaluation include **accuracy**, **hallucination rate**, and **confidence**.

## Datasets Used
- **PubMedQA**: Biomedical question-answering tasks.
- **MedQA**: Diagnostic and clinical decision-making tasks.
- **MIMIC-CXR**: Multimodal dataset combining **X-rays** and **radiology reports**.

## Technologies Used
- **Backend**: FastAPI for API development and integration with models.
- **Frontend**: HTML, CSS, and JavaScript for building the user interface.
- **Cloud Hosting**: AWS Lambda for scalable deployment.

## Contact
- [Sheikh Shapon](ssheikhorg@hotmail.com)
- [GitHub Profile](github.com/ssheikhorg)
