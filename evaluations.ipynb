{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP4jVB/g3nJsBEVbZLFiG4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssheikhorg/dissertation/blob/main/evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install uv in Colab's default environment\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "!uv --version\n",
        "\n",
        "# get python version\n",
        "!python --version\n",
        "\n",
        "!uv pip install -q -U \\\n",
        "  numpy==2.0.2 \\\n",
        "  scikit-learn==1.5.2 \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes \\\n",
        "  pillow \\\n",
        "  sentence-transformers \\\n",
        "  faiss-cpu \\\n",
        "  datasets \\\n",
        "  pandas \\\n",
        "  matplotlib \\\n",
        "  seaborn \\\n",
        "  plotly \\\n",
        "  tqdm \\\n",
        "  openai \\\n",
        "  anthropic \\\n",
        "  boto3 \\\n",
        "  langchain \\\n",
        "  langchain-huggingface \\\n",
        "  huggingface_hub \\\n",
        "  langchain-community \\\n",
        "  transformers_stream_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NerQz43cHjq9",
        "outputId": "073d2284-b739-4888-bb66-409d1bdd9317"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.8.15 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "uv 0.8.15\n",
            "Python 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Import libraries and add global variables\"\"\"\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Login to Hugging Face with your token\n",
        "login(token=userdata.get('HF_TOKEN'))\n",
        "\n",
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "# Fixed imports for LangChain compatibility\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GS9hoz7HgiQ",
        "outputId": "b3c39252-76c3-46c7-a9fc-e07278414965"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "TEMPERATURE = 0.3\n",
        "MAX_TOKENS = 1000\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ENABLE_RAG = True\n",
        "RAG_TOP_K = 3\n",
        "\n",
        "# Medical Test Data\n",
        "MEDICAL_PROMPTS = [\n",
        "    {\n",
        "        \"question\": \"What are the common symptoms of diabetes?\",\n",
        "        \"reference\": \"Common diabetes symptoms include increased thirst, frequent urination, extreme fatigue, blurred vision, and slow healing of cuts or wounds.\",\n",
        "        \"category\": \"endocrinology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How does aspirin work in the body?\",\n",
        "        \"reference\": \"Aspirin works by inhibiting cyclooxygenase enzymes, reducing the production of prostaglandins that cause pain, inflammation, and fever. It also has antiplatelet effects.\",\n",
        "        \"category\": \"pharmacology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is hypertension and what are its risk factors?\",\n",
        "        \"reference\": \"Hypertension, or high blood pressure, is a condition where the force of blood against artery walls is too high. Risk factors include age, family history, obesity, lack of exercise, tobacco use, high sodium diet, and stress.\",\n",
        "        \"category\": \"cardiology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the main functions of the liver?\",\n",
        "        \"reference\": \"The liver performs several vital functions including detoxification of chemicals, protein synthesis, production of biochemicals necessary for digestion, glycogen storage, and decomposition of red blood cells.\",\n",
        "        \"category\": \"gastroenterology\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What are the common symptoms of COVID-19?\",\n",
        "        \"reference\": \"Common COVID-19 symptoms include fever, cough, shortness of breath, fatigue, muscle aches, loss of taste or smell, sore throat, and headache.\",\n",
        "        \"category\": \"infectious_disease\"\n",
        "    }\n",
        "]\n",
        "\n",
        "MEDICAL_DATASETS = {\n",
        "    \"pubmedqa\": [\n",
        "        {\n",
        "            \"question\": \"What is the first-line treatment for hypertension?\",\n",
        "            \"reference\": \"First-line treatments for hypertension include thiazide diuretics, ACE inhibitors, angiotensin II receptor blockers, and calcium channel blockers.\",\n",
        "            \"category\": \"cardiology\",\n",
        "            \"dataset\": \"pubmedqa\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How does metformin work in type 2 diabetes?\",\n",
        "            \"reference\": \"Metformin decreases hepatic glucose production, reduces intestinal glucose absorption, and improves insulin sensitivity.\",\n",
        "            \"category\": \"endocrinology\",\n",
        "            \"dataset\": \"pubmedqa\"\n",
        "        }\n",
        "    ],\n",
        "    \"medqa\": [\n",
        "        {\n",
        "            \"question\": \"A 45-year-old patient presents with chest pain radiating to the left arm. What is the most likely diagnosis?\",\n",
        "            \"reference\": \"Chest pain radiating to the left arm is characteristic of myocardial infarction and requires immediate cardiac evaluation.\",\n",
        "            \"category\": \"cardiology\",\n",
        "            \"dataset\": \"medqa\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the gold standard test for diagnosing pulmonary embolism?\",\n",
        "            \"reference\": \"CT pulmonary angiography is the gold standard for diagnosing pulmonary embolism.\",\n",
        "            \"category\": \"pulmonology\",\n",
        "            \"dataset\": \"medqa\"\n",
        "        }\n",
        "    ],\n",
        "    \"mimic_cxr\": [\n",
        "        {\n",
        "            \"question\": \"Describe the findings in a chest X-ray showing cardiomegaly and pulmonary edema.\",\n",
        "            \"reference\": \"Cardiomegaly appears as an enlarged cardiac silhouette, while pulmonary edema manifests as bilateral interstitial opacities and Kerley B lines.\",\n",
        "            \"category\": \"radiology\",\n",
        "            \"dataset\": \"mimic_cxr\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What radiographic signs suggest pneumothorax?\",\n",
        "            \"reference\": \"Pneumothorax is characterized by a visible visceral pleural edge, absence of lung markings peripheral to this edge, and possible mediastinal shift.\",\n",
        "            \"category\": \"radiology\",\n",
        "            \"dataset\": \"mimic_cxr\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Medical knowledge base for fact checking\n",
        "MEDICAL_KNOWLEDGE_BASE = {\n",
        "    \"diabetes\": [\"increased thirst\", \"frequent urination\", \"fatigue\", \"blurred vision\", \"slow healing\", \"metformin\", \"insulin\"],\n",
        "    \"aspirin\": [\"pain relief\", \"anti-inflammatory\", \"blood thinner\", \"fever reducer\", \"inhibit cyclooxygenase\", \"myocardial infarction\"],\n",
        "    \"hypertension\": [\"high blood pressure\", \"silent killer\", \"cardiovascular risk\", \"artery damage\", \"ACE inhibitors\", \"beta blockers\"],\n",
        "    \"liver\": [\"detoxification\", \"protein synthesis\", \"bile production\", \"glycogen storage\", \"jaundice\", \"cirrhosis\"],\n",
        "    \"covid\": [\"fever\", \"cough\", \"shortness of breath\", \"loss of taste/smell\", \"coronavirus\", \"pandemic\"],\n",
        "    \"cardiology\": [\"myocardial infarction\", \"angina\", \"arrhythmia\", \"ECG\", \"troponin\", \"stent\"],\n",
        "    \"pulmonology\": [\"asthma\", \"COPD\", \"pneumonia\", \"spirometry\", \"bronchodilator\", \"oxygen therapy\"],\n",
        "    \"radiology\": [\"x-ray\", \"CT scan\", \"MRI\", \"ultrasound\", \"contrast\", \"radiation\"]\n",
        "}"
      ],
      "metadata": {
        "id": "wKca2f40IAA6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Model Configuration Functions\"\"\"\n",
        "\n",
        "def get_model_configs():\n",
        "    \"\"\"Return model configurations with their HuggingFace IDs\"\"\"\n",
        "    return {\n",
        "        \"llama-2-7b\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "        \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"qwen-7b\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        \"meditron-7b\": \"epfl-llm/meditron-7b\",\n",
        "        \"biomedgpt\": \"stanford-crfm/BioMedLM\",\n",
        "        \"gpt-oss-20b\": \"openai/gpt-oss-20b:together\",\n",
        "        \"claude-3.7-sonnet\": \"reedmayhew/claude-3.7-sonnet-reasoning-gemma3-12B\",\n",
        "        \"grok-2\": \"xai-org/grok-2\"\n",
        "    }\n",
        "\n",
        "\n",
        "def load_local_model(model_name):\n",
        "    \"\"\"Load a local model with fallback options\"\"\"\n",
        "    model_configs = get_model_configs()\n",
        "\n",
        "    if model_name not in model_configs:\n",
        "        raise ValueError(f\"Model {model_name} not supported\")\n",
        "\n",
        "    model_id = model_configs[model_name]\n",
        "\n",
        "    try:\n",
        "        # Try without quantization first\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_id,\n",
        "            use_fast=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Special handling for Qwen models\n",
        "        if \"qwen\" in model_name.lower():\n",
        "            # Qwen models require specific padding token setup\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            # Qwen models might need specific settings\n",
        "            tokenizer.padding_side = \"left\"  # Important for Qwen\n",
        "\n",
        "            # Ensure pad_token_id is properly set\n",
        "            if tokenizer.pad_token_id is None:\n",
        "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        # For non-Qwen models, set padding token if not set\n",
        "        elif tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "\n",
        "        # Load model with proper configuration\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            quantization_config=quantization_config if torch.cuda.is_available() else None,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            trust_remote_code=True,\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "\n",
        "        # Special post-processing for Qwen models\n",
        "        if \"qwen\" in model_name.lower():\n",
        "            # Ensure model is properly configured for Qwen\n",
        "            model.config.pad_token_id = tokenizer.pad_token_id\n",
        "            if hasattr(model, 'transformer'):\n",
        "                model.transformer.padding_idx = tokenizer.pad_token_id\n",
        "\n",
        "        # Create pipeline with appropriate settings for Qwen\n",
        "        generation_kwargs = {\n",
        "            \"max_new_tokens\": 384,\n",
        "            \"temperature\": TEMPERATURE,\n",
        "            \"do_sample\": True,\n",
        "            \"truncation\": True,\n",
        "        }\n",
        "\n",
        "        # Only add padding parameters if they are properly set\n",
        "        if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:\n",
        "            generation_kwargs[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "        if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:\n",
        "            generation_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            **generation_kwargs\n",
        "        )\n",
        "\n",
        "        print(f\"Successfully loaded {model_name}\")\n",
        "        return pipe\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name}: {str(e)}\")\n",
        "        # Return a proper fallback function that accepts the same parameters\n",
        "        def fallback_pipeline(prompt, **kwargs):\n",
        "            return [{'generated_text': f\"Model {model_name} could not be loaded: {str(e)}\"}]\n",
        "        return fallback_pipeline\n",
        "\n",
        "def load_model_by_category(model_name):\n",
        "    \"\"\"Load model based on its category\"\"\"\n",
        "    categories = get_model_categories()\n",
        "\n",
        "    if model_name in categories[\"local_models\"]:\n",
        "        return load_local_model(model_name)\n",
        "    else:\n",
        "        raise ValueError(f\"Model {model_name} not found in any category\")"
      ],
      "metadata": {
        "id": "DE_V2rEVIEUq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# RAG Functions\"\"\"\n",
        "def create_medical_retriever():\n",
        "    \"\"\"Create a medical knowledge retriever for RAG\"\"\"\n",
        "    medical_knowledge = [\n",
        "        \"Diabetes symptoms include increased thirst, frequent urination, fatigue, blurred vision.\",\n",
        "        \"Aspirin is a nonsteroidal anti-inflammatory drug that reduces pain and inflammation.\",\n",
        "        \"Hypertension (high blood pressure) is a condition where blood pressure is consistently too high.\",\n",
        "        \"The liver performs detoxification, protein synthesis, and produces biochemicals for digestion.\",\n",
        "        \"COVID-19 symptoms include fever, cough, shortness of breath, fatigue, and loss of taste/smell.\",\n",
        "        \"Antibiotics treat bacterial infections but are ineffective against viral infections.\",\n",
        "        \"Vaccines stimulate the immune system to produce antibodies against specific diseases.\",\n",
        "        \"Cancer treatments include surgery, chemotherapy, radiation therapy, and immunotherapy.\",\n",
        "        \"Heart disease risk factors include high blood pressure, high cholesterol, smoking, and diabetes.\",\n",
        "        \"Mental health conditions like depression can be treated with therapy and medication.\"\n",
        "    ]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500, chunk_overlap=50\n",
        "    )\n",
        "    documents = text_splitter.create_documents(medical_knowledge)\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        "    )\n",
        "\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "    return vectorstore\n"
      ],
      "metadata": {
        "id": "79HqXdhcIHRh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2TGLh_uxCKeG"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Response Generation Functions\"\"\"\n",
        "def generate_response_local(model, prompt, use_rag=True):\n",
        "    \"\"\"Generate response using local model with error handling\"\"\"\n",
        "    try:\n",
        "        if use_rag:\n",
        "            vectorstore = create_medical_retriever()\n",
        "            docs = vectorstore.similarity_search(prompt, k=RAG_TOP_K)\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            prompt_template = \"\"\"You are a medical AI assistant. Use the following medical context to answer the question accurately and factually.\n",
        "            If you don't know the answer based on the context, say you don't know. Be concise and avoid speculation.\n",
        "\n",
        "            Medical Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Answer:\"\"\"\n",
        "\n",
        "            formatted_prompt = prompt_template.format(context=context, question=prompt)\n",
        "        else:\n",
        "            formatted_prompt = prompt\n",
        "\n",
        "        # Clear memory before generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Generate response with proper error handling\n",
        "        try:\n",
        "            # Try the standard approach first\n",
        "            generation_args = {\n",
        "                'max_new_tokens': 384,\n",
        "                'do_sample': True,\n",
        "                'temperature': TEMPERATURE,\n",
        "                'truncation': True,\n",
        "            }\n",
        "\n",
        "            # Add padding token ID if available\n",
        "            if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'pad_token_id'):\n",
        "                generation_args['pad_token_id'] = model.tokenizer.pad_token_id\n",
        "\n",
        "            response = model(formatted_prompt, **generation_args)[0]['generated_text']\n",
        "\n",
        "        except Exception as gen_error:\n",
        "            print(f\"Generation error: {str(gen_error)}\")\n",
        "            # Try alternative approach without padding\n",
        "            try:\n",
        "                response = model(\n",
        "                    formatted_prompt,\n",
        "                    max_length=512,\n",
        "                    do_sample=True,\n",
        "                    temperature=TEMPERATURE,\n",
        "                    truncation=True\n",
        "                )[0]['generated_text']\n",
        "            except Exception as alt_error:\n",
        "                print(f\"Alternative generation also failed: {str(alt_error)}\")\n",
        "                return f\"Error in text generation: {str(alt_error)}\"\n",
        "\n",
        "        # Extract answer if using RAG\n",
        "        if use_rag and \"Answer:\" in response:\n",
        "            response = response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        # Clear memory after generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generation: {str(e)}\")\n",
        "        # Try to recover memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "def generate_response_cloud(model_name, prompt, use_rag=True):\n",
        "    \"\"\"Generate response using cloud model with optional RAG\"\"\"\n",
        "    try:\n",
        "        if use_rag:\n",
        "            vectorstore = create_medical_retriever()\n",
        "            docs = vectorstore.similarity_search(prompt, k=RAG_TOP_K)\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            prompt_template = \"\"\"You are a medical AI assistant. Use the following medical context to answer the question accurately and factually.\n",
        "            If you don't know the answer based on the context, say you don't know. Be concise and avoid speculation.\n",
        "\n",
        "            Medical Context:\n",
        "            {context}\n",
        "\n",
        "            Question: {question}\n",
        "\n",
        "            Answer:\"\"\"\n",
        "\n",
        "            formatted_prompt = prompt_template.format(context=context, question=prompt)\n",
        "        else:\n",
        "            formatted_prompt = prompt\n",
        "\n",
        "        response = query_cloud_model(model_name, formatted_prompt)\n",
        "        return response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generation: {str(e)}\")\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "def generate_response(model, prompt, use_rag=True, model_name=\"\"):\n",
        "    \"\"\"Generate response using local model only\"\"\"\n",
        "    return generate_response_local(model, prompt, use_rag)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"# Evaluation Functions\"\"\"\n",
        "\n",
        "def calculate_semantic_similarity(reference, response):\n",
        "    \"\"\"Calculate semantic similarity between reference and response\"\"\"\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform([reference, response])\n",
        "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "        return max(similarity, 0)\n",
        "    except:\n",
        "        return 0.5\n",
        "\n",
        "def check_factual_consistency(response, category):\n",
        "    \"\"\"Check factual consistency with medical knowledge base\"\"\"\n",
        "    if category not in MEDICAL_KNOWLEDGE_BASE:\n",
        "        return 0.7\n",
        "\n",
        "    relevant_facts = MEDICAL_KNOWLEDGE_BASE[category]\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    fact_matches = sum(1 for fact in relevant_facts if fact in response_lower)\n",
        "\n",
        "    if not relevant_facts:\n",
        "        return 0.5\n",
        "\n",
        "    coverage = fact_matches / len(relevant_facts)\n",
        "\n",
        "    contradictions = check_contradictions(response_lower, category)\n",
        "\n",
        "    final_score = coverage * (1 - 0.5 * contradictions)\n",
        "\n",
        "    return max(min(final_score, 1.0), 0.0)\n",
        "\n",
        "def check_contradictions(response, category):\n",
        "    \"\"\"Check for contradictions with known medical facts\"\"\"\n",
        "    contradiction_patterns = {\n",
        "        \"diabetes\": [r\"diabetes.*curable\", r\"diabetes.*not serious\", r\"insulin.*addictive\"],\n",
        "        \"aspirin\": [r\"aspirin.*safe for everyone\", r\"aspirin.*no side effects\", r\"aspirin.*cures\"],\n",
        "        \"covid\": [r\"covid.*just flu\", r\"vaccines.*dangerous\", r\"masks.*don't work\"],\n",
        "        \"cancer\": [r\"cancer.*always fatal\", r\"alternative.*cures cancer\", r\"chemotherapy.*poison\"]\n",
        "    }\n",
        "\n",
        "    if category not in contradiction_patterns:\n",
        "        return 0.0\n",
        "\n",
        "    patterns = contradiction_patterns[category]\n",
        "    contradiction_count = sum(1 for pattern in patterns if re.search(pattern, response))\n",
        "\n",
        "    return min(contradiction_count / len(patterns), 1.0) if patterns else 0.0\n",
        "\n",
        "def pattern_based_hallucination_detection(response):\n",
        "    \"\"\"Fallback hallucination detection using patterns\"\"\"\n",
        "    score = 0.0\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    uncertainty_patterns = [\n",
        "        r\"\\b(I think|I believe|probably|maybe|perhaps|likely)\\b\",\n",
        "        r\"\\b(studies show|research indicates|experts say)\\b\",\n",
        "    ]\n",
        "\n",
        "    for pattern in uncertainty_patterns:\n",
        "        matches = re.findall(pattern, response_lower)\n",
        "        score += len(matches) * 0.1\n",
        "\n",
        "    overgeneralizations = re.findall(r\"\\b(always|never|every|all|none)\\b\", response_lower)\n",
        "    score += len(overgeneralizations) * 0.15\n",
        "\n",
        "    sensational_claims = re.findall(r\"\\b(cure|miracle|breakthrough|revolutionary)\\b\", response_lower)\n",
        "    score += len(sensational_claims) * 0.2\n",
        "\n",
        "    return min(score, 1.0)\n",
        "\n",
        "def calculate_confidence(response):\n",
        "    \"\"\"Calculate confidence score based on response characteristics\"\"\"\n",
        "    confidence = 1.0\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    uncertainty_markers = [\"maybe\", \"perhaps\", \"I think\", \"I believe\", \"probably\"]\n",
        "    for marker in uncertainty_markers:\n",
        "        if marker in response_lower:\n",
        "            confidence -= 0.1\n",
        "\n",
        "    if len(response.split()) < 5:\n",
        "        confidence -= 0.2\n",
        "\n",
        "    return max(confidence, 0.1)\n",
        "\n",
        "def evaluate_hallucination(reference, response, category):\n",
        "    \"\"\"Evaluate hallucination using multiple methods\"\"\"\n",
        "    similarity_score = calculate_semantic_similarity(reference, response)\n",
        "\n",
        "    factual_score = check_factual_consistency(response, category)\n",
        "\n",
        "    pattern_score = pattern_based_hallucination_detection(response)\n",
        "\n",
        "    hallucination_score = 0.4 * (1 - similarity_score) + 0.4 * (1 - factual_score) + 0.2 * pattern_score\n",
        "\n",
        "    return min(hallucination_score, 1.0)\n",
        "\n",
        "def load_test_prompts(sample_count=3, dataset_name=\"all\"):\n",
        "    \"\"\"Load test prompts for evaluation from specified datasets - IMPROVED\"\"\"\n",
        "    prompts = []\n",
        "\n",
        "    if dataset_name == \"all\":\n",
        "        # Sample from ALL medical datasets\n",
        "        for dataset_key, dataset_prompts in MEDICAL_DATASETS.items():\n",
        "            samples_to_take = min(sample_count, len(dataset_prompts))\n",
        "            for i, prompt_data in enumerate(dataset_prompts[:samples_to_take]):\n",
        "                prompts.append({\n",
        "                    \"original_prompt\": prompt_data[\"question\"],\n",
        "                    \"clean_prompt\": prompt_data[\"question\"],\n",
        "                    \"original_reference\": prompt_data[\"reference\"],\n",
        "                    \"clean_reference\": prompt_data[\"reference\"],\n",
        "                    \"category\": prompt_data[\"category\"],\n",
        "                    \"dataset\": prompt_data[\"dataset\"]\n",
        "                })\n",
        "        # Also include some basic medical prompts\n",
        "        medical_samples = min(sample_count, len(MEDICAL_PROMPTS))\n",
        "        for i, prompt_data in enumerate(MEDICAL_PROMPTS[:medical_samples]):\n",
        "            prompts.append({\n",
        "                \"original_prompt\": prompt_data[\"question\"],\n",
        "                \"clean_prompt\": prompt_data[\"question\"],\n",
        "                \"original_reference\": prompt_data[\"reference\"],\n",
        "                \"clean_reference\": prompt_data[\"reference\"],\n",
        "                \"category\": prompt_data[\"category\"],\n",
        "                \"dataset\": \"medical_qa\"\n",
        "            })\n",
        "    elif dataset_name in MEDICAL_DATASETS:\n",
        "        # Sample from specific medical dataset\n",
        "        dataset_prompts = MEDICAL_DATASETS[dataset_name]\n",
        "        samples_to_take = min(sample_count, len(dataset_prompts))\n",
        "        for i, prompt_data in enumerate(dataset_prompts[:samples_to_take]):\n",
        "            prompts.append({\n",
        "                \"original_prompt\": prompt_data[\"question\"],\n",
        "                \"clean_prompt\": prompt_data[\"question\"],\n",
        "                \"original_reference\": prompt_data[\"reference\"],\n",
        "                \"clean_reference\": prompt_data[\"reference\"],\n",
        "                \"category\": prompt_data[\"category\"],\n",
        "                \"dataset\": prompt_data[\"dataset\"]\n",
        "            })\n",
        "    else:\n",
        "        # Fallback to original medical prompts\n",
        "        samples_to_take = min(sample_count, len(MEDICAL_PROMPTS))\n",
        "        for i, prompt_data in enumerate(MEDICAL_PROMPTS[:samples_to_take]):\n",
        "            prompts.append({\n",
        "                \"original_prompt\": prompt_data[\"question\"],\n",
        "                \"clean_prompt\": prompt_data[\"question\"],\n",
        "                \"original_reference\": prompt_data[\"reference\"],\n",
        "                \"clean_reference\": prompt_data[\"reference\"],\n",
        "                \"category\": prompt_data[\"category\"],\n",
        "                \"dataset\": \"medical_qa\"\n",
        "            })\n",
        "\n",
        "    return prompts\n",
        "\n",
        "def evaluate_model_responses(model, prompts, model_name, dataset=\"medical_qa\"):\n",
        "    \"\"\"Evaluate model responses for hallucinations with better error handling\"\"\"\n",
        "    results = {\n",
        "        \"model\": model_name,\n",
        "        \"dataset\": dataset,\n",
        "        \"sample_count\": len(prompts),\n",
        "        \"metrics\": {},\n",
        "        \"dataset_metrics\": {},\n",
        "        \"sample_responses\": []\n",
        "    }\n",
        "\n",
        "    hallucination_scores = []\n",
        "    accuracy_scores = []\n",
        "    confidence_scores = []\n",
        "\n",
        "    for prompt in tqdm(prompts, desc=f\"Evaluating {model_name}\"):\n",
        "        try:\n",
        "            response = generate_response(model, prompt[\"clean_prompt\"], use_rag=ENABLE_RAG, model_name=model_name)\n",
        "\n",
        "            if response.startswith(\"Error:\"):\n",
        "                accuracy = 0.0\n",
        "                hallucination_score = 0.5\n",
        "                confidence = 0.1\n",
        "            else:\n",
        "                accuracy = calculate_semantic_similarity(prompt[\"clean_reference\"], response)\n",
        "                hallucination_score = evaluate_hallucination(prompt[\"clean_reference\"], response, prompt[\"category\"])\n",
        "                confidence = calculate_confidence(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating prompt: {str(e)}\")\n",
        "            response = f\"Error: {str(e)}\"\n",
        "            accuracy = 0.0\n",
        "            hallucination_score = 0.5\n",
        "            confidence = 0.1\n",
        "\n",
        "        accuracy_scores.append(accuracy)\n",
        "        hallucination_scores.append(hallucination_score)\n",
        "        confidence_scores.append(confidence)\n",
        "\n",
        "        results[\"sample_responses\"].append({\n",
        "            \"prompt\": prompt[\"original_prompt\"],\n",
        "            \"reference\": prompt[\"original_reference\"],\n",
        "            \"response\": response,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"hallucination_score\": hallucination_score,\n",
        "            \"confidence\": confidence\n",
        "        })\n",
        "\n",
        "    if accuracy_scores and hallucination_scores:\n",
        "        results[\"metrics\"] = {\n",
        "            \"accuracy\": np.mean(accuracy_scores),\n",
        "            \"hallucination_rate\": np.mean(hallucination_scores),\n",
        "            \"confidence\": np.mean(confidence_scores),\n",
        "            \"response_length\": np.mean([len(str(r[\"response\"])) for r in results[\"sample_responses\"]]),\n",
        "            \"consistency\": 1.0 - np.std(hallucination_scores) if len(hallucination_scores) > 1 else 1.0\n",
        "        }\n",
        "\n",
        "        # Calculate metrics by dataset\n",
        "        dataset_metrics = {}\n",
        "        for prompt in prompts:\n",
        "            dataset_name = prompt.get(\"dataset\", \"unknown\")\n",
        "            if dataset_name not in dataset_metrics:\n",
        "                dataset_metrics[dataset_name] = {\n",
        "                    \"accuracy_scores\": [],\n",
        "                    \"hallucination_scores\": [],\n",
        "                    \"confidence_scores\": []\n",
        "                }\n",
        "\n",
        "        for i, response_data in enumerate(results[\"sample_responses\"]):\n",
        "            dataset_name = prompts[i].get(\"dataset\", \"unknown\")\n",
        "            dataset_metrics[dataset_name][\"accuracy_scores\"].append(response_data[\"accuracy\"])\n",
        "            dataset_metrics[dataset_name][\"hallucination_scores\"].append(response_data[\"hallucination_score\"])\n",
        "            dataset_metrics[dataset_name][\"confidence_scores\"].append(response_data[\"confidence\"])\n",
        "\n",
        "        # Compute average metrics for each dataset\n",
        "        for dataset_name, metrics in dataset_metrics.items():\n",
        "            results[\"dataset_metrics\"][dataset_name] = {\n",
        "                \"accuracy\": np.mean(metrics[\"accuracy_scores\"]) if metrics[\"accuracy_scores\"] else 0,\n",
        "                \"hallucination_rate\": np.mean(metrics[\"hallucination_scores\"]) if metrics[\"hallucination_scores\"] else 0,\n",
        "                \"confidence\": np.mean(metrics[\"confidence_scores\"]) if metrics[\"confidence_scores\"] else 0,\n",
        "                \"sample_count\": len(metrics[\"accuracy_scores\"])\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "def generate_improvement_suggestions(metrics):\n",
        "    \"\"\"Generate suggestions based on evaluation results\"\"\"\n",
        "    suggestions = []\n",
        "\n",
        "    hallucination_rate = metrics.get(\"hallucination_rate\", 0)\n",
        "    accuracy = metrics.get(\"accuracy\", 0)\n",
        "    confidence = metrics.get(\"confidence\", 0)\n",
        "    consistency = metrics.get(\"consistency\", 0)\n",
        "\n",
        "    if hallucination_rate > 0.3:\n",
        "        suggestions.append({\n",
        "            \"category\": \"High Priority\",\n",
        "            \"suggestion\": \"Implement RAG with verified medical knowledge base\",\n",
        "            \"expected_impact\": \"40-60% reduction in hallucinations\"\n",
        "        })\n",
        "\n",
        "    if accuracy < 0.6:\n",
        "        suggestions.append({\n",
        "            \"category\": \"High Priority\",\n",
        "            \"suggestion\": \"Fine-tune with curated medical QA pairs and implement fact-checking\",\n",
        "            \"expected_impact\": \"30-50% accuracy improvement\"\n",
        "        })\n",
        "\n",
        "    if confidence < 0.6:\n",
        "        suggestions.append({\n",
        "            \"category\": \"Medium Priority\",\n",
        "            \"suggestion\": \"Add confidence calibration and uncertainty quantification\",\n",
        "            \"expected_impact\": \"Better reliability estimation and fewer overconfident errors\"\n",
        "        })\n",
        "\n",
        "    if consistency < 0.7:\n",
        "        suggestions.append({\n",
        "            \"category\": \"Medium Priority\",\n",
        "            \"suggestion\": \"Implement response consistency checks and self-verification\",\n",
        "            \"expected_impact\": \"More consistent and reliable responses\"\n",
        "        })\n",
        "\n",
        "    suggestions.append({\n",
        "        \"category\": \"General\",\n",
        "        \"suggestion\": \"Implement multi-step verification: claim extraction → fact checking → response generation\",\n",
        "        \"expected_impact\": \"Overall quality and reliability improvement\"\n",
        "    })\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "\"\"\"# Model Evaluation Functions\"\"\"\n",
        "\n",
        "def evaluate_local_model(model_name, sample_count=3):\n",
        "    \"\"\"Evaluate a local model\"\"\"\n",
        "    print(f\"Evaluating local model: {model_name}...\")\n",
        "\n",
        "    try:\n",
        "        model_pipeline = load_local_model(model_name)\n",
        "\n",
        "        prompts = load_test_prompts(sample_count)\n",
        "\n",
        "        results = evaluate_model_responses(model_pipeline, prompts, model_name)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"error\": str(e),\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": 0,\n",
        "                \"hallucination_rate\": 1.0,\n",
        "                \"confidence\": 0,\n",
        "                \"response_length\": 0,\n",
        "                \"consistency\": 0\n",
        "            },\n",
        "            \"evaluation_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "def evaluate_single_model(model_name, sample_count=3):\n",
        "    \"\"\"Evaluate a single local model\"\"\"\n",
        "    results = evaluate_local_model(model_name, sample_count)\n",
        "\n",
        "    # NEW: Evaluate on ALL medical datasets\n",
        "    dataset_results = {}\n",
        "    for dataset_name in [\"pubmedqa\", \"medqa\", \"mimic_cxr\"]:\n",
        "        try:\n",
        "            dataset_eval = evaluate_model_on_dataset(model_name, dataset_name, sample_count)\n",
        "            dataset_results[dataset_name] = dataset_eval\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating on {dataset_name}: {str(e)}\")\n",
        "            dataset_results[dataset_name] = {\n",
        "                \"error\": str(e),\n",
        "                \"dataset\": dataset_name\n",
        "            }\n",
        "\n",
        "    baseline = {\n",
        "        \"accuracy\": 0.7,\n",
        "        \"hallucination_rate\": 0.25,\n",
        "        \"fact_score\": 0.75\n",
        "    }\n",
        "\n",
        "    if \"error\" not in results:\n",
        "        hallucination_reduction = (\n",
        "            ((baseline[\"hallucination_rate\"] - results[\"metrics\"][\"hallucination_rate\"]) / baseline[\"hallucination_rate\"] * 100)\n",
        "            if baseline[\"hallucination_rate\"] > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        accuracy_improvement = (\n",
        "            ((results[\"metrics\"][\"accuracy\"] - baseline[\"accuracy\"]) / baseline[\"accuracy\"] * 100)\n",
        "            if baseline[\"accuracy\"] > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        response_data = {\n",
        "            \"model\": model_name,\n",
        "            \"dataset\": \"combined_medical\",  # Changed from \"medical_qa\"\n",
        "            \"sample_count\": sample_count,\n",
        "            \"metrics\": results[\"metrics\"],\n",
        "            \"dataset_metrics\": dataset_results,  # NEW: Include all dataset results\n",
        "            \"baseline\": baseline,\n",
        "            \"improvement\": {\n",
        "                \"hallucination_reduction\": hallucination_reduction,\n",
        "                \"accuracy_improvement\": accuracy_improvement,\n",
        "            },\n",
        "            \"suggestions\": generate_improvement_suggestions(results[\"metrics\"]),\n",
        "            \"sample_responses\": results.get(\"sample_responses\", [])[:3],\n",
        "            \"evaluation_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return response_data\n",
        "    else:\n",
        "        return results\n",
        "\n",
        "def evaluate_all_models(sample_count=3):\n",
        "    \"\"\"Evaluate all available models\"\"\"\n",
        "    model_configs = get_model_configs()\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name in model_configs.keys():\n",
        "        results = evaluate_single_model(model_name, sample_count)\n",
        "        all_results[model_name] = results\n",
        "\n",
        "        with open(f\"{model_name}_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results for {model_name} saved to {model_name}_results.json\")\n",
        "\n",
        "    with open(\"all_model_results.json\", \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(\"All results saved to all_model_results.json\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def evaluate_models_by_category(category, sample_count=3):\n",
        "    \"\"\"Evaluate models by category\"\"\"\n",
        "    categories = get_model_categories()\n",
        "\n",
        "    if category not in categories:\n",
        "        raise ValueError(f\"Unknown category: {category}\")\n",
        "\n",
        "    model_names = categories[category]\n",
        "    results = {}\n",
        "\n",
        "    for model_name in model_names:\n",
        "        if category == \"local_models\":\n",
        "            results[model_name] = evaluate_local_model(model_name, sample_count)\n",
        "        else:\n",
        "            results[model_name] = evaluate_cloud_model(model_name, sample_count)\n",
        "\n",
        "    with open(f\"{category}_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"Results for {category} saved to {category}_results.json\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_model_on_dataset(model_name, dataset_name, sample_count=3):\n",
        "    \"\"\"Evaluate a model on a specific dataset\"\"\"\n",
        "    print(f\"Evaluating {model_name} on {dataset_name} dataset...\")\n",
        "    try:\n",
        "        # Load local model\n",
        "        model = load_local_model(model_name)\n",
        "\n",
        "        # Load test prompts for specific dataset\n",
        "        prompts = load_test_prompts(sample_count, dataset_name)\n",
        "\n",
        "        # Evaluate model\n",
        "        results = evaluate_model_responses(model, prompts, model_name, dataset_name)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name} on {dataset_name}: {str(e)}\")\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"dataset\": dataset_name,\n",
        "            \"error\": str(e),\n",
        "            \"metrics\": {\n",
        "                \"accuracy\": 0,\n",
        "                \"hallucination_rate\": 1.0,\n",
        "                \"confidence\": 0,\n",
        "                \"response_length\": 0,\n",
        "                \"consistency\": 0\n",
        "            },\n",
        "            \"evaluation_date\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "def evaluate_all_models_on_datasets(sample_count=3):\n",
        "    \"\"\"Evaluate all models on all datasets\"\"\"\n",
        "    model_configs = get_model_configs()\n",
        "    all_results = {}\n",
        "\n",
        "    for model_name in model_configs.keys():\n",
        "        model_results = {}\n",
        "\n",
        "        for dataset_name in [\"pubmedqa\", \"medqa\", \"mimic_cxr\", \"medical_qa\"]:\n",
        "            results = evaluate_model_on_dataset(model_name, dataset_name, sample_count)\n",
        "            model_results[dataset_name] = results\n",
        "\n",
        "        # Also evaluate on all datasets combined\n",
        "        combined_results = evaluate_single_model(model_name, sample_count)\n",
        "        model_results[\"combined\"] = combined_results\n",
        "\n",
        "        all_results[model_name] = model_results\n",
        "\n",
        "        # Save individual model results\n",
        "        with open(f\"{model_name}_dataset_results.json\", \"w\") as f:\n",
        "            json.dump(model_results, f, indent=2)\n",
        "        print(f\"Results for {model_name} saved to {model_name}_dataset_results.json\")\n",
        "\n",
        "    # Save all results together\n",
        "    with open(\"all_models_dataset_results.json\", \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    print(\"All dataset results saved to all_models_dataset_results.json\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\"\"\"# Visualization Functions\"\"\"\n",
        "\n",
        "def create_bar_chart_base64(model_results, metric=\"accuracy\"):\n",
        "    \"\"\"Create bar chart and return as base64 string\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v}\n",
        "\n",
        "    if not valid_models:\n",
        "        return None\n",
        "\n",
        "    models = list(valid_models.keys())\n",
        "    values = [valid_models[model][\"metrics\"].get(metric, 0) for model in models]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    bars = ax.bar(models, values, color=['#4CAF50', '#2196F3', '#FF9800', '#E91E63', '#9C27B0'])\n",
        "\n",
        "    # Customize the chart\n",
        "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
        "    ax.set_title(f'Model Comparison - {metric.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(values):\n",
        "        ax.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to base64\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_base64"
      ],
      "metadata": {
        "id": "jCOYEP9TIa7B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# evaluate data, visualization\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import base64\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_ui_export_data(model_results, model_name):\n",
        "    \"\"\"Create the specialized UI export data that includes medical datasets\"\"\"\n",
        "    if \"error\" in model_results:\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"error\": model_results[\"error\"],\n",
        "            \"evaluation_date\": model_results.get(\"evaluation_date\", datetime.now().isoformat())\n",
        "        }\n",
        "\n",
        "    # Extract sample responses for UI display\n",
        "    sample_responses = []\n",
        "    for i, response_data in enumerate(model_results.get(\"sample_responses\", [])[:5]):\n",
        "        sample_responses.append({\n",
        "            \"id\": i + 1,\n",
        "            \"prompt\": response_data[\"prompt\"],\n",
        "            \"reference\": response_data[\"reference\"],\n",
        "            \"response\": response_data[\"response\"],\n",
        "            \"accuracy\": response_data[\"accuracy\"],\n",
        "            \"hallucination_score\": response_data[\"hallucination_score\"],\n",
        "            \"dataset\": response_data.get(\"dataset\", \"medical_qa\")  # NEW: Include dataset info\n",
        "        })\n",
        "\n",
        "    # Extract dataset metrics for UI\n",
        "    dataset_performance = {}\n",
        "    for dataset_name, dataset_data in model_results.get(\"dataset_metrics\", {}).items():\n",
        "        if \"metrics\" in dataset_data:\n",
        "            dataset_performance[dataset_name] = dataset_data[\"metrics\"]\n",
        "\n",
        "    # Create the UI export data structure\n",
        "    ui_export_data = {\n",
        "        \"model\": model_name,\n",
        "        \"evaluation_date\": model_results.get(\"evaluation_date\", datetime.now().isoformat()),\n",
        "        \"metrics\": model_results.get(\"metrics\", {}),\n",
        "        \"dataset_metrics\": dataset_performance,  # NEW: Include dataset performance\n",
        "        \"dataset_details\": model_results.get(\"dataset_metrics\", {}),  # NEW: Full dataset details\n",
        "        \"sample_responses\": sample_responses,\n",
        "        \"suggestions\": model_results.get(\"suggestions\", []),\n",
        "        \"improvement\": model_results.get(\"improvement\", {}),\n",
        "        \"baseline\": model_results.get(\"baseline\", {})\n",
        "    }\n",
        "\n",
        "    return ui_export_data\n",
        "\n",
        "\n",
        "def create_visualization_directory_structure(model_name):\n",
        "    \"\"\"Create directory structure for storing visualization files - FIXED STRUCTURE\"\"\"\n",
        "    # Create a unified directory structure\n",
        "    base_dir = f\"model_evaluation_{model_name}\"\n",
        "\n",
        "    sub_dirs = {\n",
        "        'charts': f\"{base_dir}/charts\",\n",
        "        'data': f\"{base_dir}/data\",\n",
        "        'tables': f\"{base_dir}/tables\",\n",
        "        'dataset_analysis': f\"{base_dir}/dataset_analysis\"\n",
        "    }\n",
        "\n",
        "    # Create all directories\n",
        "    for dir_path in sub_dirs.values():\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        print(f\"Created directory: {dir_path}\")\n",
        "\n",
        "    return base_dir, sub_dirs\n",
        "\n",
        "def export_visualizations_to_directories(model_results, model_name):\n",
        "    \"\"\"Export all visualizations to organized directory structure - FIXED VERSION\"\"\"\n",
        "    try:\n",
        "        # Handle different input types\n",
        "        if isinstance(model_results, dict) and model_name in model_results:\n",
        "            model_data = model_results[model_name]\n",
        "        elif isinstance(model_results, dict) and len(model_results) == 1:\n",
        "            model_data = next(iter(model_results.values()))\n",
        "        else:\n",
        "            model_data = model_results\n",
        "\n",
        "        # Check if we have valid data\n",
        "        if not isinstance(model_data, dict) or \"error\" in model_data:\n",
        "            print(f\"Error: Invalid model data for {model_name}\")\n",
        "            return None, []\n",
        "\n",
        "        base_dir, sub_dirs = create_visualization_directory_structure(model_name)\n",
        "        exported_files = []\n",
        "\n",
        "        # Create and save UI export data\n",
        "        ui_export_data = create_ui_export_data(model_data, model_name)\n",
        "        ui_export_filename = f\"{sub_dirs['data']}/{model_name}_ui_export_data.json\"\n",
        "        with open(ui_export_filename, \"w\") as f:\n",
        "            json.dump(ui_export_data, f, indent=2)\n",
        "        exported_files.append(ui_export_filename)\n",
        "\n",
        "        # Create and save comprehensive results\n",
        "        comprehensive_filename = f\"{sub_dirs['data']}/{model_name}_comprehensive_results.json\"\n",
        "        with open(comprehensive_filename, \"w\") as f:\n",
        "            json.dump(model_data, f, indent=2)\n",
        "        exported_files.append(comprehensive_filename)\n",
        "\n",
        "        # For single model, create a dict format for visualization functions\n",
        "        model_results_dict = {model_name: model_data}\n",
        "\n",
        "        # Bar charts for key metrics\n",
        "        for metric in [\"accuracy\", \"hallucination_rate\", \"confidence\"]:\n",
        "            img_data = create_bar_chart_base64(model_results_dict, metric)\n",
        "            if img_data:\n",
        "                filename = f\"{sub_dirs['charts']}/{metric}_bar_chart.png\"\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(base64.b64decode(img_data))\n",
        "                exported_files.append(filename)\n",
        "                print(f\"Created chart: {filename}\")\n",
        "\n",
        "        # Radar chart\n",
        "        radar_img = create_radar_chart_base64(model_results_dict)\n",
        "        if radar_img:\n",
        "            filename = f\"{sub_dirs['charts']}/radar_chart.png\"\n",
        "            with open(filename, \"wb\") as f:\n",
        "                f.write(base64.b64decode(radar_img))\n",
        "            exported_files.append(filename)\n",
        "            print(f\"Created chart: {filename}\")\n",
        "\n",
        "        # Comparison table\n",
        "        comparison_table = create_comparison_table(model_results_dict)\n",
        "        if not comparison_table.empty:\n",
        "            html_filename = f\"{sub_dirs['tables']}/comparison_table.html\"\n",
        "            with open(html_filename, \"w\") as f:\n",
        "                f.write(comparison_table.to_html(classes='table table-striped', index=False))\n",
        "            exported_files.append(html_filename)\n",
        "            print(f\"Created table: {html_filename}\")\n",
        "\n",
        "        # Additional visualizations for medical datasets if available\n",
        "        if \"dataset_metrics\" in model_data and model_data[\"dataset_metrics\"]:\n",
        "            # Dataset comparison chart\n",
        "            dataset_chart = create_dataset_comparison_chart(model_results_dict)\n",
        "            if dataset_chart:\n",
        "                filename = f\"{sub_dirs['charts']}/dataset_comparison_chart.png\"\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(base64.b64decode(dataset_chart))\n",
        "                exported_files.append(filename)\n",
        "                print(f\"Created chart: {filename}\")\n",
        "\n",
        "            # Individual dataset radar charts\n",
        "            for dataset_name in [\"pubmedqa\", \"medqa\", \"mimic_cxr\"]:\n",
        "                if dataset_name in model_data.get(\"dataset_metrics\", {}):\n",
        "                    dataset_radar = create_dataset_radar_chart(model_results_dict, dataset_name)\n",
        "                    if dataset_radar:\n",
        "                        filename = f\"{sub_dirs['charts']}/{dataset_name}_radar_chart.png\"\n",
        "                        with open(filename, \"wb\") as f:\n",
        "                            f.write(base64.b64decode(dataset_radar))\n",
        "                        exported_files.append(filename)\n",
        "                        print(f\"Created chart: {filename}\")\n",
        "\n",
        "        print(f\"Exported {len(exported_files)} files to {base_dir}/ directory structure\")\n",
        "        return base_dir, exported_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in export_visualizations_to_directories: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, []\n",
        "\n",
        "def create_zip_from_directory(base_dir):\n",
        "    \"\"\"Create ZIP archive from directory structure - SIMPLIFIED\"\"\"\n",
        "    zip_filename = f\"{base_dir}.zip\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(base_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Add file to zip with relative path\n",
        "                arcname = os.path.relpath(file_path, base_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"Created ZIP archive: {zip_filename}\")\n",
        "    return zip_filename\n",
        "\n",
        "def create_radar_chart_base64(model_results):\n",
        "    \"\"\"Create radar chart comparing multiple metrics across models - UPDATED for single model\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v}\n",
        "\n",
        "    # Handle single model case by creating a minimal comparison\n",
        "    if len(valid_models) == 1:\n",
        "        # For single model, create a radar chart with just that model\n",
        "        model_name, results = next(iter(valid_models.items()))\n",
        "        metrics = ['accuracy', 'confidence', 'consistency']\n",
        "        labels = ['Accuracy', 'Confidence', 'Consistency']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "        values = [results[\"metrics\"].get(metric, 0) for metric in metrics]\n",
        "        values += values[:1]  # Close the radar chart\n",
        "\n",
        "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "        angles += angles[:1]\n",
        "\n",
        "        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color='#4CAF50')\n",
        "        ax.fill(angles, values, alpha=0.1, color='#4CAF50')\n",
        "\n",
        "        ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title(f'{model_name} Performance Radar Chart', size=14, fontweight='bold')\n",
        "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert to base64\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        return img_base64\n",
        "\n",
        "    elif len(valid_models) >= 2:\n",
        "        # Original multi-model code\n",
        "        metrics = ['accuracy', 'confidence', 'consistency']\n",
        "        labels = ['Accuracy', 'Confidence', 'Consistency']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
        "\n",
        "        colors = ['#4CAF50', '#2196F3', '#FF9800', '#E91E63', '#9C27B0']\n",
        "\n",
        "        for i, (model_name, results) in enumerate(valid_models.items()):\n",
        "            values = [results[\"metrics\"].get(metric, 0) for metric in metrics]\n",
        "            values += values[:1]  # Close the radar chart\n",
        "\n",
        "            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "            angles += angles[:1]\n",
        "\n",
        "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])\n",
        "            ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
        "\n",
        "        ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_title('Model Performance Radar Chart', size=14, fontweight='bold')\n",
        "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Convert to base64\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "        img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        return img_base64\n",
        "\n",
        "    return None\n",
        "\n",
        "def create_dataset_comparison_chart(model_results):\n",
        "    \"\"\"Create chart comparing performance across medical datasets - UPDATED\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v and \"dataset_metrics\" in v}\n",
        "\n",
        "    if not valid_models:\n",
        "        return None\n",
        "\n",
        "    # Get all medical dataset names\n",
        "    dataset_names = [\"pubmedqa\", \"medqa\", \"mimic_cxr\"]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Accuracy by dataset\n",
        "    for model_name, results in valid_models.items():\n",
        "        accuracies = []\n",
        "        for dataset in dataset_names:\n",
        "            if dataset in results.get(\"dataset_metrics\", {}):\n",
        "                accuracies.append(results[\"dataset_metrics\"][dataset].get(\"metrics\", {}).get(\"accuracy\", 0))\n",
        "            else:\n",
        "                accuracies.append(0)\n",
        "\n",
        "        axes[0].plot(dataset_names, accuracies, 'o-', label=model_name, linewidth=2, markersize=8)\n",
        "\n",
        "    axes[0].set_title('Accuracy Across Medical Datasets', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_ylim(0, 1)\n",
        "\n",
        "    # Hallucination rate by dataset\n",
        "    for model_name, results in valid_models.items():\n",
        "        hall_rates = []\n",
        "        for dataset in dataset_names:\n",
        "            if dataset in results.get(\"dataset_metrics\", {}):\n",
        "                hall_rates.append(results[\"dataset_metrics\"][dataset].get(\"metrics\", {}).get(\"hallucination_rate\", 0))\n",
        "            else:\n",
        "                hall_rates.append(0)\n",
        "\n",
        "        axes[1].plot(dataset_names, hall_rates, 'o-', label=model_name, linewidth=2, markersize=8)\n",
        "\n",
        "    axes[1].set_title('Hallucination Rate Across Medical Datasets', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Hallucination Rate')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to base64\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_base64\n",
        "\n",
        "\n",
        "def create_dataset_radar_chart(model_results, dataset_name):\n",
        "    \"\"\"Create radar chart for a specific dataset\"\"\"\n",
        "    valid_models = {k: v for k, v in model_results.items() if \"error\" not in v and \"dataset_metrics\" in v}\n",
        "\n",
        "    if not valid_models or dataset_name not in next(iter(valid_models.values()))[\"dataset_metrics\"]:\n",
        "        return None\n",
        "\n",
        "    metrics = ['accuracy', 'confidence']\n",
        "    labels = ['Accuracy', 'Confidence']\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "    colors = ['#4CAF50', '#2196F3', '#FF9800', '#E91E63', '#9C27B0']\n",
        "\n",
        "    for i, (model_name, results) in enumerate(valid_models.items()):\n",
        "        if dataset_name in results[\"dataset_metrics\"]:\n",
        "            values = [results[\"dataset_metrics\"][dataset_name].get(metric, 0) for metric in metrics]\n",
        "            values += values[:1]  # Close the radar chart\n",
        "\n",
        "            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "            angles += angles[:1]\n",
        "\n",
        "            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])\n",
        "            ax.fill(angles, values, alpha=0.1, color=colors[i % len(colors)])\n",
        "\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(f'Performance on {dataset_name.upper()} Dataset', size=14, fontweight='bold')\n",
        "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Convert to base64\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "    plt.close()\n",
        "\n",
        "    return img_base64\n",
        "\n",
        "def create_comparison_table(model_results):\n",
        "    \"\"\"Create comprehensive comparison table\"\"\"\n",
        "    data = []\n",
        "    for model_name, results in model_results.items():\n",
        "        if \"error\" not in results:\n",
        "            data.append({\n",
        "                \"Model\": model_name,\n",
        "                \"Accuracy\": f\"{results['metrics'].get('accuracy', 0):.3f}\",\n",
        "                \"Hallucination Rate\": f\"{results['metrics'].get('hallucination_rate', 0):.3f}\",\n",
        "                \"Confidence\": f\"{results['metrics'].get('confidence', 0):.3f}\",\n",
        "                \"Response Length\": f\"{results['metrics'].get('response_length', 0):.1f}\",\n",
        "                \"Consistency\": f\"{results['metrics'].get('consistency', 0):.3f}\",\n",
        "                \"Sample Count\": results.get('sample_count', 0)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "5Zh6lTQvITWt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "sample_count = 2\n",
        "def clear_gpu_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "# Call this before evaluating each model\n",
        "clear_gpu_cache()\n",
        "# all_results = evaluate_all_models(sample_count=3)\n",
        "\n",
        "# For single model export [\"llama-2-7b\", \"mistral-7b\", \"qwen-7b\", \"meditron-7b\", \"biomedgpt\", \"grok-2\", \"claude-3.7-sonnet\", \"gpt-oss-20b\"]\n",
        "model_name = \"gpt-oss-20b\"\n",
        "model_result = evaluate_single_model(model_name)\n",
        "\n",
        "base_dir, exported_files = export_visualizations_to_directories(\n",
        "    model_result, model_name\n",
        ")\n",
        "\n",
        "# Print what files were created\n",
        "print(\"Files created:\")\n",
        "for file in exported_files:\n",
        "    print(f\"  - {file}\")\n",
        "\n",
        "zip_filename = create_zip_from_directory(base_dir)\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "FJ5uLKKWCMbM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}