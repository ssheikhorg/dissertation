# Project Configuration File
project:
  name: "CDN4-Applicability-Domains"
  author: "Your Name"
  version: "1.0.0"
  description: "Evaluation of Generative AI Models' Applicability Domains"

# Model API Configuration
models:
  openai:
    api_key: "your-openai-api-key"  # Replace with actual key
    default_model: "gpt-4"
    temperature: 0.7
    max_tokens: 1000
    rate_limit_delay: 0.5  # seconds between calls

  anthropic:
    api_key: "your-anthropic-api-key"  # Replace with actual key
    default_model: "claude-3-opus-20240229"
    temperature: 0.7
    max_tokens: 1000
    rate_limit_delay: 1.0

  google:
    api_key: "your-google-api-key"  # Replace with actual key
    default_model: "gemini-pro"
    temperature: 0.7
    max_tokens: 1000
    rate_limit_delay: 2.0

  huggingface:
    cache_dir: "./hf_cache"
    default_models:
      llama2: "meta-llama/Llama-2-7b-chat-hf"
      mistral: "mistralai/Mistral-7B-v0.1"
    device: "auto"  # auto/cpu/cuda

# Evaluation Parameters
evaluation:
  datasets:
    - name: "truthful_qa"
      split: "validation"
      sample_size: 100
    - name: "winobias"
      split: "test"
      sample_size: 50
    - name: "hellaswag"
      split: "validation"
      sample_size: 200

  metrics:
    core:
      - "accuracy"
      - "rougeL"
      - "exact_match"
    bias:
      - "gender"
      - "racial"
      - "political"
    safety:
      - "toxicity"
      - "harmfulness"

  thresholds:
    acceptable_accuracy: 0.75
    max_toxicity: 0.15
    min_consistency: 0.8

# Visualization Settings
visualization:
  theme: "plotly_dark"
  color_palette: "Set2"
  default_figsize: [12, 6]
  save_format: "png"  # png/pdf/svg
  save_dpi: 300
  output_dir: "./reports/"

# Execution Control
settings:
  max_workers: 4  # For parallel evaluation
  cache_responses: true
  log_level: "INFO"  # DEBUG/INFO/WARNING/ERROR
  save_intermediate: true